#!/usr/bin/python
#coding: utf8
#Filename: Routines.py
# By:
#    Soufiane Belharbi.
#    soufiane.belharbi@litislab.fr
#    Oct. 2014

from __future__ import division
import sys
from scipy import misc
import scipy as sc
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib import rcParams
from matplotlib.patches import Rectangle
from scipy.integrate import simps, trapz
import os


import numpy as np
import scipy.io as sio
from PIL import Image
from PIL import ImageDraw
from oct2py import octave
import cv2


import theano
import theano.tensor as T


import time
import cPickle as pickle
import gzip
from xml.etree.ElementTree import Element, SubElement, Comment, tostring
from xml.etree import ElementTree
from xml.dom import minidom

#####
# Routines
#####

class Routines():
    ''' Different functions.
    
    '''
    def __init__(self):
        pass

    def generate_brodatz_texture(self, nbr_ims, s, text_back, text_for, path):
        ''' Generate Brodatz texture using only texture 77 for background, and texture 17 for forground. 
        
        The texture is generated by creatig binary masks as circles.
        
        inuput: nbr_ims: number of images to generate. 
        s: size of the squared image (s,s). 
        text_back: texture of the background. 
        text_for: texture of the forground. 
        path: path where to save the images (to check visually that everything is fine)
        The output is a texture image, and its label.
        '''
        
        
        # matrix of data
        data = np.zeros((nbr_ims, np.square(s)))
        data_labels = np.zeros((nbr_ims,np.square(s))) 
        y,x = np.ogrid[0:s, 0:s]
        for i in range(0,nbr_ims):
            # generate the OUTSIDE binary mask
            # 1. generate the outside cirlce radius
            out_r = np.random.random_integers( 20, np.divide(s,2)-2)
            # 2. generate the center
            c_h = np.random.random_integers(out_r + 1, s - out_r - 1)
            c_w = np.random.random_integers(out_r + 1, s - out_r -1)

            # 3. generate the mask
            mask_out = np.zeros((s,s))
            mask = np.square(x- c_w) + np.square(y - c_h) <= np.square(out_r)
            mask_out[mask] = 1
            
            
            # generate the INSIDE binary mask
            # 1. generate the inside circle radius
            in_r = np.random.random_integers(5, out_r - 5)
            
            # 2. generate the mask
            mask_in = np.ones((s,s))
            mask = np.square(x - c_w) + np.square(y - c_h) <= np.square(in_r)
            mask_in[mask] = 0

            # combine the INSIDE and the OUSIDE masks
            whole_mask = np.multiply(mask_out, mask_in) # element-wise multiplication
            
            # apply the mask on the forground/background image
            for_g = np.multiply(whole_mask, text_for)
            inv_mask = np.array(np.logical_not(whole_mask), dtype=int)
            back_g = np.multiply(inv_mask, text_back)    
            
            im = (for_g + back_g)/255 # normalization in [0,1]
            labels = whole_mask
            
            # saving ...
            data[i,:] = np.reshape(im, (1,np.square(s)))
            data_labels[i,:] = np.reshape(labels, (1,np.square(s)))

             # saving physically
            temp = np.concatenate((im, labels), axis=1)
            temp_im =sc.misc.toimage(temp)
            sc.misc.imsave(path + str(i) +'.png', temp_im)
        
        return data, data_labels

        
    # Mean square error
    def MeanSquareError(self,y, y_hat):
        '''Calculate the mean square error between the real value (y) and the predicted value (y_hat).'''

        '''Return MSE = 1/N \sum^N_{i=1} (y-y_hat)^2.'''
        dif =  np.subtract(y, y_hat)
        sum = np.mean(np.power(dif, 2), axis = 1)
        mse =  np.mean(sum)
        return mse
    
    def MeanSquareErrorT(self,y, y_hat):
        '''Calculate the mean square error between the real value (y) and the predicted value (y_hat).'''

        '''Return MSE = 1/N \sum^N_{i=1} (y-y_hat)^2.'''
        dif =  y - y_hat
        sum = (T.sqr(dif)).sum(axis = 1)
        mse =  T.mean(sum)
        return mse
        

    def reproject_shape(self, bboxes, model_output, nfids):
        """Reproject the predicted shape into the original plan.

        The predicted shape is reference in a plance with size of window. The unit in this plan is half size the windown (horizontal and vertical).  

        :type bboxes: matrix
        :param bboxes: the bounding boxes. Each row is a box [x,y,w,h]

        :type model_output: matrix
        :param model_output: the output of the model (regression). Each row is a shape [x,Y].
        :type window: list
        
        :type nfids: int
        :param nfids: number of landmarks

        """

        nbr = bboxes.shape[0]
        dim = model_output.shape[1]
        reprojected_shapes = np.zeros((nbr, dim))

        for i in xrange(nbr):
            bbox = bboxes[i]
            x = bbox[0]
            y = bbox[1]
            w = bbox[2]
            h = bbox[3]
            center_x = x + w/2
            center_y = y + h/2
            
            X = model_output[i, 0:nfids]
            Y = model_output[i, nfids:]

            # reprojecting ...
            X = X * (w / 2.) + center_x
            Y = Y * (h / 2.) + center_y
            
            reprojected_shapes[i] = np.concatenate((X,Y))

        
        return reprojected_shapes

    def reproject_one_shape( self, shape, bbox, window, nfids):
            '''Re-project a shape to the original plan.

            '''

            shape_re = shape
            std_w, std_h = window
            x = bbox[0]
            y = bbox[1]
            w = bbox[2]
            h = bbox[3]
            center_x = x + np.divide(w, 2)
            center_y = y + np.divide(h, 2)
            
            X = shape[0:nfids]
            Y = shape[nfids:]    
            # reprojecting ...
            X = X * (std_w / 2.) + center_x
            Y = Y * (std_h / 2.) + center_y
            shape_re = np.concatenate((X,Y))

            return shape_re

    def load_shapes_examples(self, path_train, path_test):
            '''Load the data given with IOA code.
            The given data is two sets: training and test sets. the x is grey images of a shape (disc with a whole in the middle)
            over a background. The y is the segmentation (1 for the disc, 0 for the background).
            '''
            # Train
            train_set = sio.loadmat(path_train)
            train_set_x = train_set['x_train']
            train_set_y = train_set['y_train']

            # Test
            test_set = sio.loadmat(path_test)
            test_set_x = test_set['x_test']
            test_set_y = test_set['y_test']

            return [(train_set_x, train_set_y), (test_set_x, test_set_y)]

    def filter_only_5_pts(self, phis, base_name):
            '''Take only 5 points from the face landmarks.

            The five points are: left eye, right eye, noise, left mouth, right mouth.

            :type phis: nparray
            :param phis: landmarks locations

            :type base_name: string
            :param base_name: name of the dataset
            '''
            phis_5 = np.array((phis.shape[0], 5 * 2))
            if base_name == 'cofw':
                nfids = 29
                lands=[17,18,21,23,24] - 1 # left eye, right eye, tip nose, left mouth, right mouth
            elif base_name == 'bioid':
                nfids = 20
                lands = [1, 0, 14, 3,2] # left eye, right eye, tip nose, left mouth, right mouth
            '''
            for i in xrange(5):
                    phis_5[:, i] = phis[:,lands[i]]
                    phis_5[:, i + 5] = phis[:, lands[ i + nfids]]
                    '''
            lands = (np.asarray(lands) ).tolist() # we start counting from 0
            all_lands = lands + (np.asarray(lands) + nfids).tolist()
            phis_5 = phis[:, all_lands]
            return phis_5
                    
                                      
                    
            
    def load_landmarks(self, path, base_name, window=[], force_window=False, train=True):
        """Load the data of the landmarks (converting from .mat to python format [numpy]).

        :type path: string
        :param path: path to the .mat file that contains the data.

        :type base_name: string
        :param base_name: the name of the dataset

        :type window: list
        :param window: [std_w, std_h], the width and the hieght found in the training data (the maximum). We use the same on the test data to get the same input for the model.

            :type force_window: boolean
            :param force_window: if True, we force the size of the window to be as 'window' variables even in training. This will allow us to control the size of the window as we want.
            
        :type train: boolean
        :param train: indices if it's training or test process.

        """
        mat = sio.loadmat(path)
        if train:
            iss = mat['IsTr']
            phiss = mat['phisTr']
            bboxess = mat['bboxesTr']
        else:
            iss = mat['IsT']
            phiss = mat['phisT']
            bboxess = mat['bboxesT']
        images = iss
        phis = phiss
        # Crop the images in such way we take only the part inside the bounding box (we can go futher though)
        # number of images
        nbr_ims = len(iss)
        bw = bboxess[:,2:3]# all the widths
        bh = bboxess[:,-1]# all the hieghts
        mx_w = np.max(bw)
        mx_h = np.max(bh)
        
        # the standard width and hieght that every boudning box mux be.
        # we add  a margine 
        if train:
            if force_window:
                std_w = window[0]
                std_h = window[1]
            else:
                margine = 15
                std_w = mx_w + margine
                std_h = mx_h + margine
        else:
            std_w = window[0]
            std_h = window[1]

        set_x = np.zeros((nbr_ims, std_w * std_h))
        if base_name == 'cofw':
            nfids = 29 # COFW
                    
        set_y = np.zeros((nbr_ims, nfids * 2))

        
        for i in xrange(nbr_ims):
            ########
            # Image
            ########

            im = iss[i][0]
            bbox = bboxess[i] # [x,y, w,h]
            x = bbox[0]
            y = bbox[1]
            w = bbox[2]
            h = bbox[3]
            #x_upper = x + np.divide(w,2) - np.divide(std_w, 2)
            #y_upper = y + np.divide(y,2) - np.divide(std_h, 2)
            
            center_x = x + np.divide(w, 2)
            center_y = y + np.divide(h, 2)
            x_upper = center_x - np.divide(w,2)
            y_upper = center_y - np.divide(h,2)
            
            x_lower = x_upper + std_w
            y_lower = y_upper + std_h 
            # the current image
            im_pil = Image.fromarray(im)
            # cropping 
            crop = im_pil.crop((x_upper, y_upper, x_lower, y_lower))
            #crop.show()
            #raw_input('Press a key to continue')
            # back to matrix
            mtx = np.array(crop)
            # put image lines one after another
            vec = mtx.reshape((1, std_w * std_h))
            # normalize: mean=0, var=1
            vec = (vec - np.mean(vec))/np.std(vec)
            # save 
            set_x[i, :] = vec
            #########
            # Phis
            #########
            X = phiss[i, 0:nfids]
            Y = phiss[i, nfids:2*nfids]
            # distance from the center of the bounding box / the unit of the new plan 
            # the unit equals to the half of the width (horizental) or the heght (vertical)
            # x, y in [-1, 1]
            X = (X - center_x) / (std_w / 2.)
            Y = (Y - center_y) / (std_h / 2.)
            # save
            set_y[i, :] = np.concatenate((X, Y), axis=1)
        
        if train:
            window = [std_w, std_h]
        return set_x, set_y, window, bboxess, images, phis
        
    def labels(self, data_y, nlabels):
        """ Encode labels.
        
        """
        temp = np.asarray(np.zeros([data_y.shape[0], nlabels], dtype='int32'))
        for i in xrange(data_y.shape[0]):
            temp[i, data_y[i]] = 1
        
        return temp
    def shared_dataset(self, data_xy, train=False, borrow=True):
        """Load the data to the shared variables of Theano.

        Copy for once the data to the shared memory on the GPU.
        """

        data_x, data_y = data_xy
        if train:
            dim_output = 10 # case of MNIST
            data_y = np.int32(self.labels(data_y, dim_output))
        
        shared_x = theano.shared(
                np.asarray(data_x, dtype = theano.config.floatX),
                borrow=borrow)
        shared_y = theano.shared (
                np.asarray(data_y, dtype = theano.config.floatX),
                borrow=borrow)
        return shared_x, T.cast(shared_y, 'int32')
    
    def load_data(self, dataset_path, share = False):
        """Load the data set.
        

        """
        f = gzip.open(dataset_path, 'rb')
        train_set, valid_set, test_set = pickle.load(f)
        f.close()

        # share the data
        train_set_x, train_set_y = self.shared_dataset(train_set, train=True)
        valid_set_x, valid_set_y = self.shared_dataset(valid_set)
        test_set_x, test_set_y   = self.shared_dataset(test_set)
        if share:
            reval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]
        else:
            reval = [train_set, valid_set, test_set] # NON-shared data (they didn't share the data in the code Crino!!!!!)
        return reval
    
    def shared_dataset_xy(self, data_xy, nlabels = 10, train = False, task="cls", borrow=True):
        """Load the data to the shared variables of Theano.

        Copy for once the data to the shared memory on the GPU.
        """

        data_x, data_y = data_xy
        if (train) and (task=='cls'):
            data_y = np.int32(self.labels(data_y, nlabels))
        
        shared_x = theano.shared(
                np.asarray(data_x, dtype = theano.config.floatX),
                borrow=borrow)
        shared_y = theano.shared (
                np.asarray(data_y, dtype = theano.config.floatX),
                borrow=borrow)
        return shared_x, T.cast(shared_y, 'int32')
    
    def load_tr_vl_ts(self, path,nlabels, task, share=True):
        """load mnist dataset for classification.
        
        """
        f = gzip.open(path, 'rb')
        train_set, valid_set, test_set = pickle.load(f)
        f.close()
        # share the data
        train_set_x, train_set_y = self.shared_dataset_xy(train_set, nlabels = nlabels, train=True, task=task)
        valid_set_x, valid_set_y = self.shared_dataset_xy(valid_set)
        test_set_x, test_set_y   = self.shared_dataset_xy(test_set)
        if share:
            reval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]
        else:
            if task=='cls':
                train_set = (train_set[0], self.labels(train_set[1], nlabels)) # train_y
            reval = [train_set, valid_set, test_set]
        return reval
        
    def share_set_float32(self, dataset):
        '''Share a dataset of type float32 in theano shared zone.
        
        '''
        return theano.shared( np.asarray(np.float32(dataset), dtype = theano.config.floatX), borrow=True)

    def share_set_int32(self, dataset):
        '''Share a dataset of type int32 in theano shared zone.
        
        '''
        shared_set = theano.shared ( np.asarray(np.int32(dataset), dtype = theano.config.floatX), borrow=True)
        return T.cast(shared_set, 'int32')

        
    def load_iris( self, path):
        """Load the dataset iris (UCL).

        There are 4 features, and 3 classes (Iris-setosa[0], Iris-versicolor[1], Iris-virginica[2])
        """
        
        with open(path) as f:
            content = f.readlines()

        content.pop() # remove the last item (white line)
        set_x = np.zeros((len(content), 4))
        set_y = np.zeros((len(content), ) , dtype='int')
        ind = 0
        for line in content:
            line = line.replace('\n', '')
            line = line.split(",")
            feat = [float(i) for i in line[:-1]]
            clas = line[-1]
            label = -1
            if clas == 'Iris-setosa':
                label = 0
            elif clas == 'Iris-versicolor':
                label = 1
            elif clas == 'Iris-virginica':
                label = 2
            # append
            set_x[ind,:] = feat
            set_y[ind] = label
            ind  = ind + 1
        # create train/valid/test
        data0,data1,data2 = np.split(set_x,3, axis = 0)
        y0,y1,y2 = np.split(set_y,3)
        nbr = len(set_y)
        # we mix the data to avoid having consecutive classes
        mixed_set_x = np.zeros((nbr, 4))
        mixed_set_y = np.zeros((nbr,), dtype = 'int32')
        i = 0
        j = 0
        while i < nbr:
            mixed_set_x[i,:] = data0[j,:]
            mixed_set_y[i] = y0[j]
            i = i + 1
            mixed_set_x[i,:] = data1[j,:]
            mixed_set_y[i] = y1[j]
            i = i + 1
            mixed_set_x[i,:] = data2[j,:]
            mixed_set_y[i] = y2[j]
            i = i + 1
            j = j + 1
        data0, data1, data2 = np.split(mixed_set_x, 3, axis=0)
        y0,y1,y2 = np.split(mixed_set_y, 3)
        #test set
        test_set_x = data2
        test_set_y = y2

        data11, data12 = np.split(data1,2, axis=0)
        y11, y12 = np.split(y1, 2)
        #valid set
        valid_set_x = data12
        valid_set_y = y12
        #train
        train_set_x = np.concatenate((data0,data11), axis = 0)
        train_set_y = np.concatenate((y0, y11))
        
        return [(train_set_x, train_set_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)]

    def eval_classificationT( self, y, p_y):
        """Calculate the error (100 - accuracy) of the DNN in the case of classification.

        :type y: vector 
        :param y: vector (r,) of labels

        :type p_y: matrix
        :param p_y: matrix of the output of the network. Each raw is a vector of probailities (probablities of the classes)
        """

        y_ = T.argmax(p_y, axis = 1)
        # Accuracy
        error = 1 - T.mean(T.eq(y_, y) * 1.)
        error = error * 100.
        
        return error

    def eval_segmentation_bin( self, y, model_output, th=.5, path='../data/predict/'):
        '''Evaluation the performance of a binary segmentation. The default used threshold

        .5.
        The used evaluation is the mean squarre error.
        '''
        # binarization
        nbr, dim = y.shape
        output_bin = np.float32((model_output > th) * 1.)
        mse = self.MeanSquareError(y, model_output)
        for i in xrange(nbr):
            im_gt = Image.fromarray(np.reshape(np.uint8(y[i,:] *255.), (128,128)))
            im_bin = Image.fromarray(np.reshape(np.uint8(output_bin[i,:] *255.), (128,128)))
            im_gr  = Image.fromarray(np.reshape(np.uint8(model_output[i,:] *255.) , (128,128)))
            temp = np.concatenate((im_gt, im_bin, im_gr), axis=1)
            two_imgs = sc.misc.toimage(temp)
            sc.misc.imsave(path + str(i) +'.png', two_imgs)
            #two_imgs.show()
            #raw_input('Press ENTER to continue...')
        return mse
    
    def segment( self, y, model_output, th=.5, path='../data/predict/'):
        '''Segment an image using the output of a neural network. The default used threshold

        .5.
        '''
        # binarization
        nbr, dim = y.shape
        output_bin = np.float32((model_output > th) * 1.)
        for i in xrange(nbr):
            im_gt = Image.fromarray(np.reshape(np.uint8(y[i,:]) *255., (128,128)))
            im_bin = Image.fromarray(np.reshape(np.uint8(output_bin[i,:]) *255., (128,128)))
            im_gr  = Image.fromarray(np.reshape(np.uint8(model_output[i,:] *255.) , (128,128)))
            temp = np.concatenate((im_gt, im_bin, im_gr), axis=1)
            two_imgs = sc.misc.toimage(temp)
            sc.misc.imsave(path + str(i) +'.png', two_imgs)
            #two_imgs.show()
            #raw_input('Press ENTER to continue...')
            
    def uncompress_ae(self, listae, data, folder, ae_type='in'):
        """ Get the output of the hidden and the output layers of each auto-encoder in the listae.
        
        save the output as images.
        """
        if ae_type == "out": # reverse if it's the output aes.
            listae = listae[::-1]
        shared_inputs = data
        i = 0
        folder = os.path.realpath(folder)
        c_folder= folder+"/ae-"+str(i)+"/"
        for ae in listae:
            forward = ae.forwardFunction(downcast=True, shared_x_data=shared_inputs)
            output = forward()
            hidden = (ae.hiddenValues(shared_inputs.get_value())+1.)/2.
            hidden_folder = c_folder+"hidden/"
            output_folder = c_folder+"output/"
            if not os.path.exists(hidden_folder):
                os.makedirs(hidden_folder)
            if not  os.path.exists(output_folder):
                os.makedirs(output_folder)
            
            h = np.int(np.sqrt(hidden.shape[1]))
            hidden_to_save = hidden
            # in case the size has not an exact sqrt, so we add zeros, at the end.
            if np.power(h,2)  != hidden.shape[1] :
                to_add = np.power(h+1, 2) - hidden.shape[1]
                add_z = np.zeros((hidden.shape[0], to_add))
                hidden_to_save = np.concatenate((hidden, add_z), axis=1)
                hidden_to_save = np.array(hidden_to_save, dtype=theano.config.floatX)
                
                h = h + 1
                w = h
            else:
                w = h                    
            
            self.outputToimage(  model_output=hidden_to_save,h=h, w=w, path=hidden_folder)
            h = np.int(np.sqrt(output.shape[1]))
            output_to_save = output
            # in case the size has not an exact sqrt, so we add zeros, at the end.
            if np.power(h,2)  != output.shape[1] :
                to_add = np.power(h+1, 2) - output.shape[1]
                add_z = np.zeros((output.shape[0], to_add))
                output_to_save = np.concatenate((output, add_z), axis=1)
                h = h + 1
                w = h
            else:
                w = h 
            self.outputToimage(  model_output=output_to_save,h=h, w=w, path=output_folder)
            
            i +=1
            c_folder= folder+"/ae-"+str(i)+"/"
            shared_inputs=theano.shared(hidden)   
            
    def outputToimage( self, model_output,h=128, w=128, path='../data/predict/'):
        '''Convert the output of a neural network to image.
        
        '''
        
        nbr, dim = model_output.shape
        for i in xrange(nbr):
            im  = Image.fromarray(np.reshape(np.uint8(model_output[i,:] *255.) , (h,w)))
            im = sc.misc.toimage(im)
            sc.misc.imsave(path + str(i) +'.png', im) 
            
    def eval_regression( self, y, model_output, base_name  ):
        """ Calculate the performance of the regression (case of face landmarks).

        
        """
        y = np.array(y, dtype=theano.config.floatX)
        model_output = np.array(model_output, dtype=theano.config.floatX)
        octave.addpath('../')
        
        if base_name=='lfpw' or base_name =='helen': # use the given compute error function in Ibug challenge.(http://ibug.doc.ic.ac.uk/media/uploads/competitions/compute_error.m)
            nfids = np.int(y.shape[1]/2)
            n_ims = y.shape[0]
            y_reshaped = np.zeros((nfids, 2, n_ims))
            model_output_reshaped = np.zeros((nfids, 2, n_ims))
            for i in  xrange(n_ims):
                y_reshaped[:, 0, i] = y[i, :nfids] #x
                y_reshaped[:, 1, i] = y[i, nfids:] #y
                model_output_reshaped[:, 0, i] = model_output[i, :nfids] #x
                model_output_reshaped[:, 1, i] = model_output[i, nfids:] #x
            loss = octave.compute_error_ibug(y_reshaped, model_output_reshaped)
                        
        else:
            ds,dsAll = octave.dist_error(base_name, model_output, y)
            loss = ds # NRMSE: Normalized Root-Mean-Squarre
                
        
        # Error
        mu1, muAll,fail = octave.error_detection(loss)

        return mu1, muAll, fail, loss
    
    def simpson_area(sefl, y, dx):
        """ Calculate the area under a curve using the composite Simpson's rule.
        
        """
        
        return simps(y, dx=dx)
        
    def trapez_area(sefl, y, dx):
        """ Calculate the area under a curve using the composite trapezoidal rule.
        
        """
        
        return trapz(y, dx=dx)    
    def calculate_auc(self, y, dx, border_x=0.5, border_y=1.)   :
        """ calculate the Area Under the Curve using the average of the estimated areas by the two composites the Simpsons's and the trapezoidal rules.
        
        """
        AUC = (self.simpson_area(y, dx) + self.trapez_area(y, dx )) / 2.
        return AUC
        
    def plot_cdf_of_nrmse(self, loss, loss_mean_shape,x, path="../models/cdfnrmse.jpg", border_x=0.5, dx=0.001, x_0=-1):
        """ plot cdf of the NRMSE.
        
        loss is the loss vector returned by eval_regression() which is the error for each image
        loss_mean_shape: the loss when evaluating using the mean shape train.
        """
        plt.close('all')
        nbr = len(loss)
        nbr_x = len(x)
        cdf_loss = np.zeros(nbr_x)
        cdf_loss_mean_shape = np.zeros(nbr_x)
        for i in xrange(nbr_x):
            cdf_loss[i] = np.sum((loss <= x[i]) * 1.)
            cdf_loss_mean_shape[i] = np.sum((loss_mean_shape <= x[i]) * 1.)
        
        cdf_loss = cdf_loss / nbr
        cdf_loss_mean_shape = cdf_loss_mean_shape / nbr
        # calculate the area Under the curve using the average of the composites Simpson’s rule and trapezoidal rule.
        AUC_cdf_loss = self.calculate_auc(cdf_loss, dx, border_x, border_y= 1.)
        AUC_cdf_loss_mean_shape = self.calculate_auc(cdf_loss_mean_shape, dx, border_x, border_y= 1.)
        # Normalize the area by dividing it by the area of the bounding rectangle 
        complet_area= border_x * 1.
        AUC_cdf_loss = 100 * AUC_cdf_loss / complet_area
        AUC_cdf_loss_mean_shape = 100 * AUC_cdf_loss_mean_shape / complet_area
        #plt.ioff()
        fig = plt.figure()
        plt.xticks([0.01, 0.02, 0.05, 0.07, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5])
        plt.yticks([0.1, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0])
        plt.xticks(rotation=70)
        plt.grid(b=True, which='major', axis='both')
        
        floating = 3
        prec = "%." + str(floating) + "f"
        
        temp = (x <= x_0)
        temp = [x[i] for i in xrange(nbr_x) if temp[i]]
        x_0 = temp[-1]
        index = np.where(x == x_0)
        plt.plot(x,cdf_loss, color='r', linestyle="--", marker='None', label="CDF NRMSE, CDF("+str(x_0)+")="+str(prec % np.float(cdf_loss[index][0]*100))+"%, AUC="+ str(prec % np.float(AUC_cdf_loss)) + "%")
        plt.plot(x,cdf_loss_mean_shape, color='c', linestyle="-", marker='None', label="CDF NRMSE Mean shape TRAIN, CDF("+str(x_0)+")="+str(prec % np.float(cdf_loss_mean_shape[index][0]*100))+"% , AUC="+ str(prec % np.float(AUC_cdf_loss_mean_shape)) + "%")
        
        plt.plot([x_0, x_0], [0, 1], '--b', lw=1, label='x_0:'+str(x_0)+ ", AUC: Area Under the Curve [x_max="+ str(border_x)+",y_max=1]" )
        fig.suptitle('Cumulative distribution function (CDF) of NRMSE')
        plt.xlabel('NRMSE')
        plt.ylabel('Data proportion')
        plt.legend(loc=4, prop={'size':8}, fancybox=True, shadow=True)
        fig.savefig(path, bbox_inches='tight')
        plt.ion()
        plt.close('all')        
        return cdf_loss
        
    def calculate_auc_of_loss(self, loss,x, border_x=0.5, dx=0.001):
        """Calculate the AUC of a loss in a limited border.
        
        """
        nbr = len(loss)
        nbr_x = len(x)
        cdf_loss = np.zeros(nbr_x)
        for i in xrange(nbr_x):
            cdf_loss[i] = np.sum((loss <= x[i]) * 1.)
        
        cdf_loss = cdf_loss / nbr
        # calculate the area Under the curve using the average of the composites Simpson’s rule and trapezoidal rule.
        AUC_cdf_loss = self.calculate_auc(cdf_loss, dx, border_x, border_y= 1.)
        # Normalize the area by dividing it by the area of the bounding rectangle 
        complet_area= border_x * 1.
        AUC_cdf_loss = 100 * AUC_cdf_loss / complet_area
        
        return cdf_loss, AUC_cdf_loss
        
    def plot_cdfs(self, loss_mlp, loss_dnn, loss_ioda,loss_mean_shape, x, path, border_x=0.5, dx=0.001,x_0=-1):
        """Plot the CDF of the loss of different architectures already have been saved.
        
        """
        plt.close('all')
        nbr = len(loss_mlp)
        nbr_x = len(x)
        cdf_loss_mlp = np.zeros(nbr_x)
        cdf_loss_dnn = np.zeros(nbr_x)
        cdf_loss_ioda = np.zeros(nbr_x)
        cdf_loss_mean_shape = np.zeros(nbr_x)
        for i in xrange(nbr_x):
            cdf_loss_mlp[i] = np.sum((loss_mlp <= x[i]) * 1.)
            cdf_loss_dnn[i] = np.sum((loss_dnn <= x[i]) * 1.)
            cdf_loss_ioda[i] = np.sum((loss_ioda <= x[i]) * 1.)
            cdf_loss_mean_shape[i] = np.sum((loss_mean_shape <= x[i]) * 1.)
        
        cdf_loss_mlp = cdf_loss_mlp / nbr
        cdf_loss_dnn = cdf_loss_dnn / nbr
        cdf_loss_ioda = cdf_loss_ioda / nbr
        cdf_loss_mean_shape = cdf_loss_mean_shape / nbr
        # calculate the area Under the curve using the average of the composites Simpson’s rule and trapezoidal rule.
        AUC_cdf_loss_mlp = self.calculate_auc(cdf_loss_mlp, dx, border_x, border_y= 1.)
        AUC_loss_dnn = self.calculate_auc(cdf_loss_dnn, dx, border_x, border_y= 1.)
        AUC_loss_ioda = self.calculate_auc(cdf_loss_ioda, dx, border_x, border_y= 1.)
        AUC_loss_mean_shape = self.calculate_auc(cdf_loss_mean_shape, dx, border_x, border_y= 1.)
        # Normalize the area by dividing it by the area of the bounding rectangle 
        complet_area= border_x * 1.
        AUC_cdf_loss_mlp = 100 * AUC_cdf_loss_mlp / complet_area
        AUC_loss_dnn = 100 * AUC_loss_dnn / complet_area
        AUC_loss_ioda = 100 * AUC_loss_ioda / complet_area
        AUC_loss_mean_shape = 100 * AUC_loss_mean_shape / complet_area
        plt.ioff()
        fig = plt.figure()
        plt.xticks([0.01, 0.02, 0.05, 0.07, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5])
        plt.yticks([0.1, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0])
        plt.xticks(rotation=70)
        plt.grid(b=True, which='major', axis='both')
        floating = 3
        prec = "%." + str(floating) + "f"
        
        if x_0 == -1:
            plt.plot(x,cdf_loss_mlp, color='r', linestyle="--", marker=',', label="CDF NRMSE MLP, AUC="+ str(prec % np.float(AUC_cdf_loss_mlp))+ "%")
            plt.plot(x,cdf_loss_dnn, color='g', linestyle="--", marker=',', label="CDF NRMSE DNN, AUC="+ str(prec % np.float(AUC_loss_dnn))+ "%")
            plt.plot(x,cdf_loss_ioda, color='b', linestyle="--", marker=',', label="CDF NRMSE IODA, AUC="+ str(prec % np.float(AUC_loss_ioda))+ "%")
            plt.plot(x,cdf_loss_mean_shape, color='c', linestyle="-", marker=',', label="CDF NRMSE Mean shape, AUC="+ str(prec % np.float(AUC_loss_mean_shape))+ "%")
        elif x_0 > 0 and x_0 <= max(x):
            temp = (x <= x_0)
            temp = [x[i] for i in xrange(nbr_x) if temp[i]]
            x_0 = temp[-1]
            index = np.where(x == x_0)
            plt.plot(x,cdf_loss_mlp, color='r', linestyle=":", marker=',', label="CDF NRMSE DNN 0-0-0, CDF("+str(x_0)+")="+str(prec % np.float(cdf_loss_mlp[index][0]*100))+"%, AUC="+ str(prec % np.float(AUC_cdf_loss_mlp))+ "%")
            plt.plot(x,cdf_loss_dnn, color='g', linestyle="--", marker=',', label="CDF NRMSE DNN 3-0-0, CDF("+str(x_0)+")="+str(prec % np.float(cdf_loss_dnn[index][0]*100))+"%, AUC="+ str(prec % np.float(AUC_loss_dnn))+ "%")
            plt.plot(x,cdf_loss_ioda, color='b', linestyle="-", marker=',', label="CDF NRMSE IODA 1-0-3, CDF("+str(x_0)+")="+str(prec % np.float(cdf_loss_ioda[index][0]*100)) +"%, AUC="+ str(prec % np.float(AUC_loss_ioda))+ "%")
            plt.plot(x,cdf_loss_mean_shape, color='c', linestyle="-.", marker=',', label="CDF NRMSE mean shape, CDF("+str(x_0)+")="+str(prec % np.float(cdf_loss_mean_shape[index][0]*100)) +"%, AUC="+ str(prec % np.float(AUC_loss_mean_shape))+ "%")
            plt.plot([x_0, x_0], [0, 1], '-k', lw=0.5, label='x_0:'+str(x_0) + ", AUC: Area Under the Curve [x_max="+ str(border_x)+",y_max=1]" )
            
            
        plt.legend(loc=4, prop={'size':8}, fancybox=True, shadow=True)
        #fig.suptitle('Cumulative distribution function (CDF) of NRMSE')
        plt.xlabel('NRMSE')
        plt.ylabel('Data proportion')
        fig.savefig(path, bbox_inches='tight',format='eps', dpi=1000)
        plt.ion()        
        plt.close('all')
        return cdf_loss_mlp, cdf_loss_dnn, cdf_loss_ioda
    
    def plot_multiple_nrmse(self,exp_path, mean_shape_dump_path):
        """In exp_path there must be 3 folders: mlp, dnn, ioda. In each folder, there must be a file named:loss_and_others.dump which contains the loss and other stuff.
        
        
        mean_shape_dump_path: path of the dumped file that contains the evaluation using the mean shape train.
        """
        plt.close('all')
        mlp=exp_path+'mlp/loss_and_others.dump'
        dnn=exp_path+'dnn/loss_and_others.dump'
        ioda=exp_path+'ioda/loss_and_others.dump'
        list_files = [mlp, dnn, ioda]
        losses=[]
        f = open(mean_shape_dump_path, "rb")
        [mu1_mean_shape, muAll_mean_shape, fail_mean_shape, loss_mean_shape] = pickle.load(f)
        f.close()
        
        for path in list_files:
            f = open(path, 'rb')
            loaded = pickle.load(f)
            f.close()
            losses.append(loaded[3])
        border_x=0.5
        dx=0.001
        x = np.arange(0,border_x,dx)
        x_0=0.1
        path_out=exp_path+"multiple_cdfs.png"
        self.plot_cdfs(loss_mlp=losses[0], loss_dnn=losses[1], loss_ioda=losses[2],loss_mean_shape=loss_mean_shape, x=x, path=path_out, border_x=border_x, dx=dx, x_0=x_0)
        
        plt.close('all')
        return losses  
        
    def plot_multiple_nrmse_2(self,paths, mean_shape_dump_path, path_out):
        """In exp_path there must be 3 folders: mlp, dnn, ioda. In each folder, there must be a file named:loss_and_others.dump which contains the loss and other stuff.
        
        
        mean_shape_dump_path: path of the dumped file that contains the evaluation using the mean shape train.
        """
        plt.close('all')
        mlp=paths['mlp']
        dnn=paths['dnn']
        ioda=paths['ioda']
        list_files = [mlp, dnn, ioda]
        losses=[]
        f = open(mean_shape_dump_path, "rb")
        [mu1_mean_shape, muAll_mean_shape, fail_mean_shape, loss_mean_shape] = pickle.load(f)
        f.close()
        
        for path in list_files:
            f = open(path, 'rb')
            loaded = pickle.load(f)
            f.close()
            losses.append(loaded[3])
        border_x=0.5
        dx=0.001
        x = np.arange(0,border_x,dx)
        x_0=0.1
        
        self.plot_cdfs(loss_mlp=losses[0], loss_dnn=losses[1], loss_ioda=losses[2],loss_mean_shape=loss_mean_shape, x=x, path=path_out, border_x=border_x, dx=dx, x_0=x_0)
        
        plt.close('all')
        return losses              
    def show_images(self, x_set, path):
        nbr = x_set.shape[0]
        for i in xrange(nbr):
            im = Image.fromarray(np.reshape(np.uint8(x_set[i,:] *255.) , (128,128)))
            im = sc.misc.toimage(im)
            sc.misc.imsave(path + str(i) +'.png', im)
    
    
    def show_shape( self, im, phis, bbox):
        """ Display a shape over the face image. (octave)
        
        """
            
        phis = np.array(phis, dtype='float32')
        bbox = np.array(bbox, dtype='float32')
        octave.addpath('./octave_files/')
        octave.Show_shape(im, phis, bbox)
        return 0
    
    def show_landmarks_points(self, im, phis_pred, phis_gt, bbox, nrmse=-1 ,save=False, path="../im.png"):
        """ Display a shape over the face image. (python)
        
                
        phis_pred: predicted phis [xxxyyy]
        phis_gt: phis of ground of truth
        bbox=[x y w h]
        im = np.ndarray
        nrmse: float (nrmse between gt and pred)
        """
        plt.close('all')
        if save:
            plt.ioff()
             
        nfids = int(len(phis_pred)/2)
        plt.imshow(im, cmap = cm.Greys_r)
        
        
        if nrmse > 0:
            gt = plt.scatter(x=phis_gt[0:nfids], y=phis_gt[nfids:], c='g', s=40)
            pr = plt.scatter(x=phis_pred[0:nfids], y=phis_pred[nfids:], c='r', s=20)
            plt.legend((gt, pr), ("grund truth", "prediction, nrmse="+str(nrmse)), scatterpoints=1, fontsize=8, bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
                       ncol=2, mode="expand", borderaxespad=0., fancybox=True, shadow=True)
        else:       # just display the landmarks of an image.   gt=pred
            
            gt = plt.scatter(x=phis_gt[0:nfids], y=phis_gt[nfids:], c='g', s=20)
            plt.legend([gt], ["landmarks"], scatterpoints=1, fontsize=8, bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
                       ncol=1, mode="expand", borderaxespad=0., fancybox=True, shadow=True)
        """
        plt.plot([bbox[0], bbox[0]],[bbox[1],bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0], bbox[0]+bbox[2]],[bbox[1], bbox[1]],'-b', linewidth=1)
        plt.plot([bbox[0]+bbox[2], bbox[0]+bbox[2]],[bbox[1], bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0] ,bbox[0]+bbox[2]],[bbox[1]+bbox[3] ,bbox[1]+bbox[3]],'-b', linewidth=1)
        """
        plt.axis('off')
        
        if save:
            plt.savefig(path,bbox_inches='tight', dpi=100)
            plt.ion()
        else:
            plt.show()
            raw_input("... Press ENTER to continue,")
            
        plt.close('all')
        
    def show_landmarks_segments(self, im, phis_pred, phis_gt, bbox,nrmse, save=False, path="../im.png"):
        """ Display a shape over the face image. (python)
        
                
        phis_pred: predicted phis [xxxyyy]
        phis_gt: phis of ground of truth
        bbox=[x y w h]
        im = np.ndarray
        nrmse: float (nrmse between gt and pred)
        """
        plt.close('all')
        if save:
            plt.ioff()
             
        nfids = int(len(phis_pred)/2)
        plt.imshow(im, cmap = cm.Greys_r)
        x_gt = phis_gt[:nfids]
        y_gt = phis_gt[nfids:]
        x_pred = phis_pred[:nfids]
        y_pred = phis_pred[nfids:]
        for i in xrange(nfids):
            plt.plot([x_gt[i], x_pred[i]],[y_gt[i], y_pred[i]], '-r')
        
        
        # Use only the segments
#        leg1 = Rectangle((0, 0), 0, 0, alpha=0.0)
#        plt.legend([leg1],['Segments between gt and pred. NRMSE= '+str(nrmse)], scatterpoints=1, fontsize=8, bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
#           ncol=2, mode="expand", borderaxespad=0.)
        # use the segments with small points.
        gt = plt.scatter(x=phis_gt[0:nfids], y=phis_gt[nfids:], c='g', s=0)
        pr = plt.scatter(x=phis_pred[0:nfids], y=phis_pred[nfids:], c='r', s=0)
        plt.legend((gt, pr), ("grund truth", "prediction, nrmse="+str(nrmse)), scatterpoints=1, fontsize=8, bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
           ncol=2, mode="expand", borderaxespad=0., fancybox=True, shadow=True)
        """
        plt.plot([bbox[0], bbox[0]],[bbox[1],bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0], bbox[0]+bbox[2]],[bbox[1], bbox[1]],'-b', linewidth=1)
        plt.plot([bbox[0]+bbox[2], bbox[0]+bbox[2]],[bbox[1], bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0] ,bbox[0]+bbox[2]],[bbox[1]+bbox[3] ,bbox[1]+bbox[3]],'-b', linewidth=1)
        """
        plt.axis('off')
        
        if save:
            plt.savefig(path,bbox_inches='tight', dpi=100)
            plt.ion()
        else:
            plt.show()
            raw_input("... Press ENTER to continue,")
            
        plt.close('all')
        
    def show_landmarks_unit_test(self, im, phis_pred, phis_mean_train, bbox, save=False, path="../im.png"):
        """ Display a shape over the face image. (python)
        
                
        phis_pred: predicted phis [xxxyyy]
        phis_mean_train: mean phis of ground of truth
        bbox=[x y w h]
        im = np.ndarray
        """
        plt.close('all')
        if save:
            plt.ioff()
             
        nfids = int(len(phis_pred)/2)
        plt.imshow(im, cmap = cm.Greys_r)
        gt = plt.scatter(x=phis_mean_train[0:nfids], y=phis_mean_train[nfids:], c='g', s=40)
        pr = plt.scatter(x=phis_pred[0:nfids], y=phis_pred[nfids:], c='r', s=20)
        
        mse = np.mean(np.power((phis_pred - phis_mean_train), 2))
        plt.legend((gt, pr), ("mean shape train", "prediction, MSE="+str(mse)), scatterpoints=1,loc='lower left', fontsize=8, fancybox=True, shadow=True)
        """
        plt.plot([bbox[0], bbox[0]],[bbox[1],bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0], bbox[0]+bbox[2]],[bbox[1], bbox[1]],'-b', linewidth=1)
        plt.plot([bbox[0]+bbox[2], bbox[0]+bbox[2]],[bbox[1], bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0] ,bbox[0]+bbox[2]],[bbox[1]+bbox[3] ,bbox[1]+bbox[3]],'-b', linewidth=1)
        """
        plt.axis('off')
        
        if save:
            plt.savefig(path,bbox_inches='tight', dpi=1000)
            plt.ion()
        else:
            plt.show()
            raw_input("... Press ENTER to continue,")
            
        plt.close('all')
        
    def show_only_landmarks_unit_test(self, im, phis_pred, bbox, save=False, path="../im.png"):
        """ Display a shape over the face image. (python)
        
                
        phis_pred: predicted phis [xxxyyy]
        phis_mean_train: mean phis of ground of truth
        bbox=[x y w h]
        im = np.ndarray
        """
        plt.close('all')
        if save:
            plt.ioff()
             
        nfids = int(len(phis_pred)/2)
        plt.imshow(im, cmap = cm.Greys_r)
        pr = plt.scatter(x=phis_pred[0:nfids], y=phis_pred[nfids:], c='w', s=20)
        
        plt.legend([pr], ["prediction"], scatterpoints=1,loc='lower left', fontsize=8, fancybox=True, shadow=True)
        """
        plt.plot([bbox[0], bbox[0]],[bbox[1],bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0], bbox[0]+bbox[2]],[bbox[1], bbox[1]],'-b', linewidth=1)
        plt.plot([bbox[0]+bbox[2], bbox[0]+bbox[2]],[bbox[1], bbox[1]+bbox[3]],'-b', linewidth=1)
        plt.plot([bbox[0] ,bbox[0]+bbox[2]],[bbox[1]+bbox[3] ,bbox[1]+bbox[3]],'-b', linewidth=1)
        """
        plt.axis('off')
        
        if save:
            plt.savefig(path,bbox_inches='tight', dpi=100)
            plt.ion()
        else:
            plt.show()
            raw_input("... Press ENTER to continue,")
            
        plt.close('all')        
    def save_output_landmarks(self, phis_pred, phis_gt, bboxes, images, folder,loss, type_disp='points'):
        """ Save the output of the model.
        
        phis_pred: reprojected model output 
        phis_gt: phis ground of truth
        bboxes: bounding boxes
        images: images
        folder: folder where to save the images  
        loss: NRMSE between the groun truth and the prediction
        type_disp: how to display the landmarks. as points 'points', or the segments between gt and pred 'segments'
        """
        nbr = phis_pred.shape[0]
        
        for i in xrange(nbr):
            im = images[i][0]
            bb = bboxes[i] 
            phis_pred_ = phis_pred[i]
            phis_gt_ = phis_gt[i]
            path = folder+str(i)+".png"
            if type_disp =='points':
                self.show_landmarks_points( im=im, phis_pred=phis_pred_, phis_gt=phis_gt_, bbox=bb, nrmse=loss[i,0], save=True, path=path)
            else:
                self.show_landmarks_segments( im=im, phis_pred=phis_pred_, phis_gt=phis_gt_, bbox=bb, nrmse=loss[i,0], save=True, path=path)
                    
    def plot_errors(self, valid, train, path, epoc=-1):
        '''Plot then save the figure of the error over the valid and train sets calculated during  the gradient descent. ***** CLASSIFICATION
        
        The figure won't be displayed.  
        valid: list of errors over the validation set
        train: list of errors over the train set
        path: path where to save the figure.
        '''
        fig = plt.figure()
        train_gp, = plt.plot(train, '-r')
        valid_gp, = plt.plot(valid, '-*g')
        if epoc >= 0:
            epoc = epoc - 1 # ploting starts from 0
            stop, = plt.plot([epoc, epoc], [0, max(valid + train) + 5], '--b', lw=2)
            plt.legend([train_gp, valid_gp, stop], ['train error', 'valid error', 'stop learning, epoch='+str(epoc + 1)], fancybox=True, shadow=True)
        else:
            plt.legend([train_gp, valid_gp], ['train error', 'valid error'], fancybox=True, shadow=True)
            
        plt.title('Train/valid error during the gradient descent')
        plt.xlabel(u"n° epoch")
        plt.ylabel('Error (100 - accuracy) %')
        fig.savefig(path, bbox_inches='tight')
        # to display the figure
        #plt.show()

    def plot_ae_tracker(self, tracker, path):
        '''Plot the loss, lr of each auto-encoder.
        
        tracker: list of tuple (loss, lr)
        '''
        nbr = len(tracker)
        f, (loss, lr) = plt.subplots(2, sharex=True, sharey = False)
        # plotting
        cmap = plt.get_cmap('gnuplot')
        colors = [cmap(i) for i in np.linspace(0,1, nbr)]
        floating = 3
        prec = "%." + str(floating) + "f"

        for i, color in enumerate(colors, start=1):
            loss_ = np.asarray(tracker[i-1][0])
            end_loss =  prec % np.float(loss_[-1])
            lr_ = np.asarray(tracker[i-1][1])
            end_lr = prec % np.float(lr_[-1])
            loss.plot(loss_, color = color, label='loss ae_' + str(i) + ':' + str(end_loss))
            lr.plot(lr_, color = color, label='lr ae_'+ str(i) + ':' + str(end_lr))

        loss.legend(fancybox=True, shadow=True, prop={'size':6})
        lr.legend(fancybox=True, shadow=True, prop={'size':6})
        loss.set_title('loss and learing rate during the auo-encoders pre-training.')
        lr.set_xlabel(u'n° epoch')
        lr.set_ylabel('lambda')
        loss.set_ylabel('loss')
        f.savefig(path, bbox_inches='tight')
    
    def plot_ae_in_out(self, ae_in, ae_out,link_trackers, path, valid_status=False):
        """ Plot the loss of a list of ae (in, out).
        
        ae_in and ae_out are both a list of triplet ((loss, valid_error, best_model_epoch)).
        """
        #rcParams.update({'figure.autolayout': True})
        if valid_status == False:
            link_loss = link_trackers[0]
            if len(ae_out) > 0:
                if len(link_loss)>0:
                    f, (loss_in, loss_link, loss_out) = plt.subplots(3, sharex=True, sharey = False)
                else:
                    f, (loss_in, loss_out) = plt.subplots(2, sharex=True, sharey = False)
            else:
                if len(link_loss)>0:
                    f, (loss_in, loss_link) = plt.subplots(2, sharex=True, sharey = False)
                else:
                    f, loss_in = plt.subplots(1)
            nbr_in = len(ae_in)
            cmap = plt.get_cmap('gnuplot')
            colors =[cmap(i) for i in np.linspace(0,1, nbr_in+2)]
            colors = colors[1:-1]
            markers = [',', ',', ',', ',', ',', ',', ',',',']
            floating = 3
            prec = "%." + str(floating) + "f"
            # ae in        
            for i in xrange(nbr_in):
                color = colors[i]
                marker = markers[i]
                loss = ae_in[i][0]
                end_loss = prec % np.float(loss[-1])            
                loss_in.plot(loss, color = color, marker=marker, linestyle="--", label='loss ae-in-'+str(i)+ ':'+ str(end_loss)+ ',ep:' + str(len(loss)))
                
            if len(link_loss) >0:
                loss = link_loss
                end_loss = prec % np.float(loss[-1])            
                loss_link.plot(loss, color = "r", marker=",", linestyle="--", label='loss link:'+ str(end_loss)+ ',ep:' + str(len(loss)))
            # ae out
            nbr_out = len(ae_out)
            colors =[cmap(i) for i in np.linspace(0,1, nbr_out+2)]
            colors = colors[1:-1]
            
            for i in xrange(nbr_out):
                color = colors[i]
                marker = markers[i]
                loss = ae_out[i][0]
                end_loss = prec % np.float(loss[-1])
                loss_out.plot(loss, color = color, marker=marker, linestyle="--", label='loss ae-out-'+str(i)+ ':'+ str(end_loss) + ',ep:' + str(len(loss)))
            
            # Put a legend below current axis
            loss_in.legend(fancybox=True, shadow=True, prop={'size':6})
            #loss_in.legend()
            loss_in.set_ylabel('MSE')
            loss_in.tick_params(axis='both', which='major', labelsize=10)
            loss_in.tick_params(axis='both', which='minor', labelsize=8)
            if len(ae_out) > 0:
                if len(link_loss)>0:
                    loss_in.set_title('MSE of AEs input/output (up/bottum), link (middle) during the pre-training.', fontsize=10)
                    loss_link.set_ylabel('MSE')
                    loss_link.legend(fancybox=True, shadow=True, prop={'size':6})
                    loss_link.tick_params(axis='both', which='major', labelsize=10)
                    loss_link.tick_params(axis='both', which='minor', labelsize=8)
                else:
                    loss_in.set_title('MSE of AEs input/output (up/down) during the pre-training.', fontsize=10)
                
                loss_out.set_ylabel('MSE')
                loss_out.legend(fancybox=True, shadow=True, prop={'size':6})
                loss_out.set_xlabel(u'n° epoch')
                loss_out.tick_params(axis='both', which='major', labelsize=10)
                loss_out.tick_params(axis='both', which='minor', labelsize=8)
                
            else:
                if len(link_loss)>0:
                    loss_in.set_title('MSE of AEs input (up), link (bottum) during the pre-training.', fontsize=10)
                    loss_link.set_xlabel(u'n° epoch')
                    loss_link.set_ylabel('MSE')
                    loss_link.legend(fancybox=True, shadow=True, prop={'size':6})
                    loss_link.tick_params(axis='both', which='major', labelsize=10)
                    loss_link.tick_params(axis='both', which='minor', labelsize=8)
                else:
                    loss_in.set_title('MSE of AEs input during the pre-training.', fontsize=10)
                    loss_in.set_xlabel(u'n° epoch')
                    loss_in.set_ylabel('MSE')
            

        else:
            link_loss = link_trackers[0]
            if len(ae_out) > 0: # there is output ae
                if len(link_loss)>0: # there is link
                    f, ((loss_in, valid_in), (loss_link, valid_link), (loss_out, valid_out)) = plt.subplots(3, 2, sharex='col')
                else:
                    f, ((loss_in, valid_in), (loss_out, valid_out)) = plt.subplots(2,2, sharex='col')
            else:
                if len(link_loss)>0:
                    f, ((loss_in, valid_in), (loss_link, valid_link)) = plt.subplots(2,2, sharex='col')
                else:
                    f, ((loss_in, valid_in)) = plt.subplots(1,2, sharex='col')
                    
            nbr_in = len(ae_in)
            cmap = plt.get_cmap('gnuplot')
            colors =[cmap(i) for i in np.linspace(0,1, nbr_in+2)]
            colors = colors[1:-1]
            markers = [',', ',', ',', ',', ',', ',', ',']
            floating = 3
            prec = "%." + str(floating) + "f"
            # ae in        
            for i in xrange(nbr_in):
                color = colors[i]
                marker = markers[i]
                loss = ae_in[i][0]
                valid_error= ae_in[i][1]
                best_model_epoch = ae_in[i][2]
                optim_loss = prec % np.float(loss[best_model_epoch - 2])   
                ind_valid= best_model_epoch - (len(loss) - len(valid_error)) - 1
                best_valid = prec % np.float(valid_error[ind_valid])
                loss_in.plot(loss, color = color, marker=marker, linestyle="--", label='loss ae-in-'+str(i)+ ':'+ str(optim_loss)+ ',max ep:' + str(len(loss)))
                valid_in.plot(valid_error, color = color, marker=marker, linestyle="--", label='valid ae-in-'+str(i)+ ':'+ str(best_valid))
                loss_in.plot([best_model_epoch - 1, best_model_epoch - 1], [0, 1.1 * max(abs(loss)) *  np.sign(max(loss))], color = color, linestyle="-", lw=1, label='Best model at ep:'+str(best_model_epoch  ) )
                valid_in.plot([ind_valid, ind_valid], [0, 1.1 * max(abs(valid_error)) *  np.sign(max(valid_error))], color = color, linestyle="-", lw=1, label='Best model at ep:'+str(best_model_epoch) )
                
            if len(link_loss) >0:
                loss = link_loss
                
                # sometimes we don't want to valid the link.
                valid_error=link_trackers[1]
                best_model_epoch =link_trackers[2]
                if len(valid_error) > 0:
                    optim_loss = prec % np.float(loss[best_model_epoch - 2])   
                    ind_valid= best_model_epoch - (len(loss) - len(valid_error)) - 1
                    best_valid = prec % np.float(valid_error[ind_valid])
                    loss_link.plot(loss, color = "r", marker=",", linestyle="--", label='loss link:'+ str(optim_loss)+ ', max ep:' + str(len(loss)))
                    valid_link.plot(valid_error, color = "r", marker=",", linestyle="--", label='valid link:'+ str(best_valid))
                    loss_link.plot([best_model_epoch - 1, best_model_epoch - 1], [0, 1.1 * max(abs(loss)) *  np.sign(max(loss))], color = "r", linestyle="-", lw=1, label='Best model at ep:'+str(best_model_epoch  ) )
                    valid_link.plot([ind_valid, ind_valid], [0, 1.1 * max(abs(valid_error)) *  np.sign(max(valid_error))], color = "r", linestyle="-", lw=1, label='Best model at ep:'+str(best_model_epoch) )
                else:
                    end_loss = prec % np.float(loss[-1])            
                    loss_link.plot(loss, color = "r", marker=",", linestyle="--", label='loss link:'+ str(end_loss)+ ', max ep:' + str(len(loss)))
                    loss_link.plot([len(loss)-1, len(loss) - 1], [0, 1.1 * max(abs(loss)) *  np.sign(max(loss))], color = "r", linestyle="-", lw=1, label='Best model at ep:'+str(best_model_epoch  ) )    
                    
                    
            # ae out
            nbr_out = len(ae_out)
            colors =[cmap(i) for i in np.linspace(0,1, nbr_out+2)]
            colors = colors[1:-1]
            
            for i in xrange(nbr_out):
                color = colors[i]
                marker = markers[i]
                loss = ae_out[i][0]
                valid_error= ae_out[i][1]
                best_model_epoch = ae_out[i][2]
                optim_loss = prec % np.float(loss[best_model_epoch - 2])   
                ind_valid= best_model_epoch - (len(loss) - len(valid_error)) - 1
                best_valid = prec % np.float(valid_error[ind_valid])
                loss_out.plot(loss, color = color, marker=marker, linestyle="--", label='loss ae-out-'+str(i)+ ':'+ str(optim_loss) + ',max ep:' + str(len(loss)))
                valid_out.plot(valid_error, color = color, marker=marker, linestyle="--", label='valid ae-out-'+str(i)+ ':'+ str(best_valid))
                loss_out.plot([best_model_epoch - 1, best_model_epoch - 1], [0, 1.1 * max(abs(loss)) *  np.sign(max(loss))], color = color, linestyle="-", lw=1, label='Best model at ep:'+str(best_model_epoch ) )
                valid_out.plot([ind_valid, ind_valid], [0, 1.1 * max(abs(valid_error)) *  np.sign(max(valid_error))], color = color, linestyle="-", lw=1, label='Best model at ep:'+str(best_model_epoch) )
                
            # Put a legend below current axis
            loss_in.legend(fancybox=True, shadow=True, prop={'size':6})
            valid_in.legend(fancybox=True, shadow=True, prop={'size':6})
            #loss_in.legend()
            loss_in.set_ylabel('MSE')
            loss_in.tick_params(axis='both', which='major', labelsize=10)
            loss_in.tick_params(axis='both', which='minor', labelsize=8) 
            valid_in.tick_params(axis='both', which='major', labelsize=10)
            valid_in.tick_params(axis='both', which='minor', labelsize=8) 
            if len(ae_out) > 0:
                if len(link_loss)>0:
                    f.suptitle('MSE of AEs input/output[valid] (up/bottum), link (middle) during the pre-training.', fontsize=10)                    
                    loss_link.set_ylabel('MSE')
                    loss_link.legend(fancybox=True, shadow=True, prop={'size':6})
                    if len(link_trackers[1]) >0:
                        valid_link.legend(fancybox=True, shadow=True, prop={'size':6})
                    loss_link.tick_params(axis='both', which='major', labelsize=10)
                    loss_link.tick_params(axis='both', which='minor', labelsize=8) 
                    valid_link.tick_params(axis='both', which='major', labelsize=10)
                    valid_link.tick_params(axis='both', which='minor', labelsize=8)
                else:
                    f.suptitle('MSE of AEs input/output[valid] (up/down) during the pre-training.', fontsize=10)
                
                loss_out.set_ylabel('MSE')
                loss_out.legend(fancybox=True, shadow=True, prop={'size':6})
                valid_out.legend(fancybox=True, shadow=True, prop={'size':6})
                loss_out.set_xlabel(u'n° epoch')
                valid_out.set_xlabel(u'n° epoch')  
                loss_out.tick_params(axis='both', which='major', labelsize=10)
                loss_out.tick_params(axis='both', which='minor', labelsize=8) 
                valid_out.tick_params(axis='both', which='major', labelsize=10)
                valid_out.tick_params(axis='both', which='minor', labelsize=8) 
                
            else:
                if len(link_loss)>0:
                    f.suptitle('MSE of AEs input (up)[valid], link (bottum) during the pre-training.', fontsize=10)
                    loss_link.set_xlabel(u'n° epoch')
                    loss_link.set_ylabel('MSE')
                    valid_link.set_xlabel(u'n° epoch')
                    loss_link.legend(fancybox=True, shadow=True, prop={'size':6})
                    loss_link.tick_params(axis='both', which='major', labelsize=10)
                    loss_link.tick_params(axis='both', which='minor', labelsize=8) 
                    valid_link.tick_params(axis='both', which='major', labelsize=10)
                    valid_link.tick_params(axis='both', which='minor', labelsiz=8)
                else:
                    f.suptitle('MSE of AEs input[valid] during the pre-training.', fontsize=10)
                    loss_in.set_xlabel(u'n° epoch')
                    valid_in.set_xlabel(u'n° epoch')
            
            
          
        f.savefig(path, bbox_inches='tight')
        #rcParams.update({'figure.autolayout': False})
        
    def plot_list_ea_trakers(self, listae_in, listae_out,link_trackers, path, valid_status=False):
        """Plot the "loss, lr" of a list of auto-encoders and the link.
        
        """
        trackers_in=[]
        trackers_out=[]
        
        for ae_in in listae_in:
            epoch, lr, loss = map(list, zip(*ae_in.finetune_history))
            loss = loss[1:]
            valid_error = []
            best_model_epoch = -1
            if valid_status == True:
                valid_error = ae_in.valid_history
                best_model_epoch = ae_in.best_model_epoch + 1
            
            trackers_in.append((np.array(loss), np.array(valid_error), best_model_epoch))
            
            
        if len(listae_out) > 0:
            for ae_out in listae_out:
                epoch, lr, loss = map(list, zip(*ae_out.finetune_history))
                loss=loss[1:]
                valid_error = []
                best_model_epoch = -1
                if valid_status == True:
                    valid_error = ae_out.valid_history
                    best_model_epoch = ae_out.best_model_epoch + 1
                
                trackers_out.append((np.array(loss), np.array(valid_error), best_model_epoch))

        self.plot_ae_in_out(ae_in = trackers_in, ae_out=trackers_out, link_trackers=link_trackers, path = path, valid_status=valid_status)
            
            
    def plot_errors_adv(self, loss, lamb, path, valid_error=[], train_error=[], epoch=-1):
        '''Plot different variables during the gradient descent: loss, lambda, error valid, error train.
        
        mandatory:
            loss, lamb, path
        You can plot:   loss, lamb
                        loss, lamb, valid
                        loss, lamb, train
                        loss, lamb, train/valid
        for each simple graph (for example, loss), we plot where epoch stopped. You can provide where to plot epoch, otherwise it will use the last epoch for each graph (last element).
        
        '''
        # convert all to np.array
        loss = np.array(loss)
        lamb = np.array(lamb)
        valid_error = np.array(valid_error)
        train_error = np.array(train_error)
        #***************************
        font = {
        'size': 9}
        plt.rc('font', **font)
        shrink = 0.8 # shrink the figures to put the legends on right.
        nbr_sp = len(loss)
        if epoch == -1:
            epoch = nbr_sp - 1
        else:
            epoch = epoch - 1
        
        nbr_gs = 2 # loss and lambda are mandatory
        if len(valid_error) > 0:
            nbr_gs += 1
        if len(train_error) > 0:
            nbr_gs += 1
        
        # build lambda array if it's just one number (fixed step)
        if len(lamb) == 1:
            lamb = np.ones((nbr_sp,)) * lamb[0]

        # Graphs
        if nbr_gs == 2: # loss, lambda
            f, (loss_g, lamb_g) = plt.subplots(2, sharex=True, sharey=False)
        elif (nbr_gs == 3) and (len(valid_error)> 0): # loss, lambda, valid
            f, (loss_g, lamb_g, valid_g) = plt.subplots(3, sharex=True, sharey=False)
        elif (nbr_gs == 3) and (len(train_error) > 0): # loss, lambda, train
            f, (loss_g, lamb_g, train_g) = plt.subplots(3, sharex=True, sharey=False)
        elif nbr_gs == 4: # loss, lambda, valid, train
            f, (loss_g, lamb_g, valid_train_g) = plt.subplots(3, sharex=True, sharey=False)
        
        # mandatory
        floating = 3
        prec = "%."+str(floating) +"f"
        end_loss =  prec % np.float(loss[epoch])
        loss_g.plot(loss, 'r', label='loss ('+ end_loss+ ')')
        loss_g.plot([epoch, epoch], [0, 1.1 * max(abs(loss)) *  np.sign(max(loss))], '--b', lw=2, label='Epoch:'+str(epoch + 1) )
        #loss_g.legend()
        box = loss_g.get_position()
        loss_g.set_position([box.x0, box.y0, box.width * shrink, box.height])
        loss_g.legend(loc='center left', bbox_to_anchor=(1, 0.5), fancybox=True, shadow=True, prop={'size':6})
        loss_g.set_ylabel('loss')
        
        end_lamd = prec % np.float(lamb[epoch])
        lamb_g.plot(lamb, 'r', label='lambda ('+ end_lamd + ')')
        lamb_g.plot([epoch, epoch], [0, 1.1 * max(abs(lamb)) *  np.sign(max(lamb))], '--b', lw=2, label='Epoch:'+str(epoch + 1))
        #lamb_g.legend()
        box = lamb_g.get_position()
        lamb_g.set_position([box.x0, box.y0, box.width * shrink, box.height])
        lamb_g.legend(loc='center left', bbox_to_anchor=(1, 0.5), fancybox=True, shadow=True, prop={'size':6})
        lamb_g.set_ylabel('lambda')
        
        if nbr_gs == 2: # loss, lambda
            loss_g.set_title('loss and lambda during the gradient descent.')
            lamb_g.set_xlabel(u'n° epochs')
            
        elif (nbr_gs == 3) and (len(valid_error) > 0): # loss, lambda, valid
            ind_valid=epoch+1 - (len(loss) - len(valid_error)) - 1
            optim_valid = prec % np.float(valid_error[ind_valid])
            valid_g.plot(valid_error, 'r', label='valid error ('+ optim_valid +')')
            valid_g.plot([ind_valid, ind_valid], [0, 1.1 * max(abs(valid_error)) *  np.sign(max(valid_error))], '--b', lw=2, label='Epoch:'+str(epoch + 1))
            #valid_g.legend()
            box = valid_g.get_position()
            valid_g.set_position([box.x0, box.y0, box.width * shrink, box.height])
            valid_g.legend(loc='center left', bbox_to_anchor=(1, 0.5), fancybox=True, shadow=True, prop={'size':6})
            valid_g.set_xlabel(u'n° epochs')
            valid_g.set_ylabel('valid error  %')
            loss_g.set_title('loss, lambda and valid error during the gradient descent.')
            
        elif (nbr_gs == 3) and (len(train_error) > 0): # loss, lambda, train
            end_train = prec % np.float(train_error[epoch])
            train_g.plot(train_error, 'r', label='train error ('+ end_train +')')
            train_g.plot([epoch, epoch], [0, 1.1 * max(abs(train_error)) *  np.sign(max(train_error)) ], '--b', lw=2, label='Epoch:'+str(epoch + 1) +', tr error:'+ str(train_error[epoch]))
            #train_g.legend()
            box = train_g.get_position()
            train_g.set_position([box.x0, box.y0, box.width * shrink, box.height])
            train_g.legend(loc='center left', bbox_to_anchor=(1, 0.5), fancybox=True, shadow=True, prop={'size':6})
            train_g.set_xlabel(u'n° epochs')
            train_g.set_ylabel('train error  %')
            loss_g.set_title('loss, lambda and train error during the gradient descent.')
            
        elif nbr_gs == 4: # loss, lambda, valid, train
            ind_valid=epoch+1 - (len(loss) - len(valid_error)) - 1
            end_valid = prec % np.float(valid_error[ind_valid])
            end_train = prec % np.float(train_error[epoch])
            valid_train_g.plot(valid_error, '--g', label='valid error ('+ end_valid+')')
            valid_train_g.plot(train_error, '-r', label='train error ('+ end_train+')')
            valid_train_g.plot([epoch, epoch], [0, 1.1 * max(abs(train_error+ valid_error)) *  np.sign(max(train_error+valid_error)) ], '--b', lw=2, label='Epoch:'+str(epoch + 1))
            #valid_train_g.legend()
            box = valid_train_g.get_position()
            valid_train_g.set_position([box.x0, box.y0, box.width * shrink, box.height])
            valid_train_g.legend(loc='center left', bbox_to_anchor=(1, 0.5), fancybox=True, shadow=True, prop={'size':6})
            valid_train_g.set_xlabel(u'n° epochs')
            valid_train_g.set_ylabel('valid, train error %')
            loss_g.set_title('loss, lambda, train and valid error during the gradient descent.')
            
            
        # save
        f.savefig(path, bbox_inches='tight')
        plt.rcdefaults()
        
        
    def plot_errors_from_dump(self, path_dump):
        """From dumped file, plot then save the figure of the error over the valid and train sets calculated during  the gradient descent.
        
        the dumped file contains a tuplet of 3 elements: (error_valid, error_train, path)
        """
        (error_valid, error_train, path) = pickle.load(open(path_dump, 'rb'))
        plot_errors(error_valid, error_train, path)
        
    # TO CORRECT LATER
    def plot_errors_landmarks(self, loss, lamb, path, valid_error=[], train_error=[], epoch=-1):
        """ Plot the errors during the network training. 
        
        valid/train: list of tuplets (mu1, muAll, fail)
        """

        # validation data
        mu1_valid = [tup[0] for tup in valid_error]
        muAll_valid = [tup[1] for tup in valid_error]
        fail_valid = [tup[2] for tup in valid_error]

        
        # train data
        mu1_train = [tup[0] for tup in train_error]
        muAll_train = [tup[1] for tup in train_error]
        fail_train = [tup[2] for tup in train_error]
      
        
        f, (mu1, muAll, fail) = plt.subplots(3, sharex=True, sharey=False)
        mu1.plot(mu1_valid, '-*g', label='mu1 valid')
        mu1.plot(mu1_train, '-r', label='mu1 train')
        mu1.legend(fancybox=True, shadow=True, prop={'size':6})
        mu1.set_title('mu, muAll, fail over train/valid set during the gradient descent')
        
        muAll.plot(muAll_valid, '-*g', label='muAll valid')
        muAll.plot(muAll_train, '-r', label='muAll train')
        muAll.legend(fancybox=True, shadow=True, prop={'size':6})
        
        fail.plot(fail_valid, '-*g', label='fail valid')
        fail.plot(fail_train, '-r', label='fail train')
        fail.legend(fancybox=True, shadow=True, prop={'size':6})
        
        fail.set_xlabel(u'n° epoch')
        # save the figure
        f.savefig(path, bbox_inches='tight')

    def plot_errors_landmarks_from_dump(self, path_dump):
        """From dumped file, plot then save the figure of the error over the valid and train sets calculated during  the gradient descent.
        
        the dumped file contains a tuplet of 3 elements: (error_valid, error_train, path)
        """
        (error_valid, error_train, path) = pickle.load(open(path_dump, 'rb'))
        self.plot_errors_landmarks(error_valid, error_train, path)
        
    def construct_train_valid_sets(self, data, p=1/3.):
        ''' Construct train/valid sets from a dataset, by selecting p% of samples as valid, and the rest for train.mat
        
        data: tuple (X,Y)
        p: real ]0,1[, the pourcentage of samples to take as validation.
        
        The samples will be selected randomly.
        '''
        x = data[0]
        y = data[1]
        nbr = x.shape[0]
        nbr_vald = int(nbr * p)
        index = np.arange(nbr)
        # shuffle the index
        np.random.shuffle(index)
        idx_valid = index[:nbr_vald]
        idx_train = index[nbr_vald:]
        
        x_train = x[idx_train]
        y_train = y[idx_train]
        
        x_valid = x[idx_valid]
        y_valid = y[idx_valid]
        
        return [(x_train, y_train), (x_valid, y_valid)]
        
    def face_detect(self, img):
        """ Detect the face location of the image img, using Haar cascaded face detector of OpenCV.
        
        return : x,y w, h of the bouning box.
        """
        face_cascade = cv2.CascadeClassifier('../haarcascades/haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(img, 1.3, 5)
        x = -1
        y = -1
        w = -1
        h = -1
        if len(faces) == 1: # we take only when we have 1 face, else, we return nothing.
            x,y,w,h = faces[0]
        else:
##            for (x_,y_,w_,h_) in faces:
##                x = x_
##                y = y_
##                w = w_
##                h = h_
##                break # we take only the first face,
            print "More than one face!!!!!!!!!"
            
        
        return x,y,w,h
        
    def crop(self, im, init_bbx, window, fill="same", take_marge=0.8, add_big=50):
        """ Crop an image using the bounding box (face) and an initial (approximative) bbox. 
        
        The size of the final cropped image is in window.
        
        im: image
        init_bbx=[x,y,w,h]
        window:[w,h], TO MAKE THINGS EASIER, THE BOUNDING BOX IS A SQUARRED IMAGE (w=h)
        
        fill: how to fill the space between in-image and the window (if there is a space):
        - "same": tak the whole in-image
        - "white": fill it with white
        - "balck: fill it with black
        - "grey": fill it with grey
        - "noise": fill it with RANDOM grey noise
        
        take_marge: [0,1], how pourcentage we take from the left marge between the in-image and window, from each side.
        add_big: number of pixels to add when the face is big, so we don't stike to the boudaries of the face.
        """
        x,y,w,h= init_bbx
        center_x_init = int(x + w/2)
        center_y_init = int(y + h/2)
        
        z = max(w,h)
        # z bbox
        x_z = center_x_init - z/2
        y_z = center_y_init - z/2
        
        # 3 cases: 
        # z is inside the window (z is smaller)        
        # z is outside the window (z is bigger)
        # z is equal to the window
        if z < window[0]: # inside [we don't resize the face in this case.]
            # crop
            x_upper = int(center_x_init - window[0]/2)
            y_upper = int(center_y_init - window[1]/2)
            x_lower = int(x_upper + window[0])
            y_lower = int(y_upper + window[1])
            
            im_cropped = im.crop((x_upper, y_upper, x_lower, y_lower))
            im_cropped.load()
            # treat the space bewteen z and window
            if fill == "same":
                mtx = np.asarray(im_cropped)
                mask = mtx
            elif fill =="white":
                mask = np.ones((window[0], window[1]), dtype=np.uint8) * 255
            elif fill == "black":
                mask = np.zeros((window[0], window[1]), dtype=np.uint8)
            elif fill == "grey":
                mask = np.ones((window[0], window[1]), dtype=np.uint8) * 128
            elif fill == "noise":
                noise = np.random.rand(window[0], window[1])
                noise = (noise * 255)
                noise.astype(int)
                mask = noise
            
            bin_mask  = np.zeros((window[0], window[1]), dtype=np.uint8)
            add_x = int(take_marge * (window[0] - z))
            add_y = int(take_marge * (window[1] - z))
            s_x = int((window[0] - (z + add_x))/2)
            s_y = int((window[1] - (z + add_y))/2)
            e_x = int(s_x + z + add_x)
            e_y = int(s_y + z + add_x)
            
            for i in range(s_x, e_x+1):
                for j in range(s_y, e_y+1):
                    bin_mask[i][j] = 1
            
            inv_mask = (bin_mask == 0)* 1
            mask = np.uint8(np.multiply(inv_mask, mask))            
            mask = Image.fromarray(mask)
            im_mtx = np.asarray(im_cropped)            
            masked_im = np.uint8(np.multiply(im_mtx, bin_mask))            
            out = np.add(masked_im, mask)            
            im_cropped = Image.fromarray(np.uint8(out))            
            
            bbox = [x_upper, y_upper, window[0], window[1]]
            original = bbox
        elif z > window[0]: # outside [resize the face]
            z += add_big
            x_z = center_x_init - z/2
            y_z = center_y_init - z/2
            x_upper = int(x_z)
            y_upper = int(y_z)
            x_lower = int(x_upper + z)
            y_lower = int(y_upper + z)
            z_cropped = im.crop((x_upper, y_upper, x_lower, y_lower))
            # resize the image
            z_cropped.thumbnail((window[0], window[1]), Image.ANTIALIAS)

            
            # save the original plane
            original = [x_upper, y_upper, z, z]
            im_cropped = z_cropped.copy()
            
            x = center_x_init - window[0]/2
            y = center_y_init - window[1]/2
            bbox = [x, y, window[0], window[1]]
            
        elif z == window[0]: # equal
            x = center_x_init - window[0]/2
            y = center_y_init - window[1]/2
            
            # crop
            x_upper = x
            y_upper = y
            x_lower = x_upper + window[0]
            y_lower = y_upper + window[1]
            
            im_cropped = im.crop((x_upper, y_upper, x_lower, y_lower))
            bbox = [x_upper, y_upper, window[0], window[1]]
            original = bbox
        
        
        
        return im_cropped, bbox, original
        
    def re_size_shape(self, set_y, bbox, bbox_original):
        """resize the shapes for the images that has been resized.
        
        From ORIGINAL SIZE TO SMALLER SIZE.
        set_y:[XY], is not projected.
        
        """
        nbr= set_y.shape[0]   
        nfids = int(set_y.shape[1]/2)
        
        for i in xrange(nbr):
            box = bbox[i]
            box_ori = bbox_original[i]

            if not (box == box_ori).all(): # if resized
                XY = set_y[i]
                X=XY[:nfids]
                Y=XY[nfids:]
                
                # translate the plane center to the middle of the bbx
                center_x = box_ori[0]+ box_ori[2]/2
                center_y = box_ori[1] + box_ori[3]/2
                X = X - center_x
                Y = Y - center_y             
                prop_x = (box_ori[2] / box[2]) # w_o /w
                prop_y = (box_ori[3] / box[3]) # h_o /h
                X = X / prop_x
                Y = Y / prop_y
                # get back to the original plane
                X = X + center_x
                Y = Y + center_y
                XY=np.concatenate((X, Y))
                set_y[i] = XY
        
        return set_y
        
    def back_to_original_size_shapes(self, set_y, bbox, bbox_original):
        """back to the original size for the images that have been resize.
        
        FROM A SMALLER SIZE, TO THE ORIGINAL SIZE.
        set_y: [XY], are REAL (not in [-1,+1]), in the bbox (not the original)
        
        """
        
        nbr= set_y.shape[0]   
        nfids = int(set_y.shape[1]/2)
        
        for i in xrange(nbr):
            box = bbox[i]
            box_ori = bbox_original[i]

            if not (box == box_ori).all(): # if resized
                XY = set_y[i]
                X=XY[:nfids]
                Y=XY[nfids:]
                
                # translate the plane center to the middle of the bbx
                center_x = box_ori[0]+ box_ori[2]/2
                center_y = box_ori[1] + box_ori[3]/2
                X = X - center_x
                Y = Y - center_y             
                prop_x = (box_ori[2] / box[2]) # w_o /w
                prop_y = (box_ori[3] / box[3]) # h_o /h
                X = X * prop_x
                Y = Y * prop_y
                # get back to the original plane
                X = X + center_x
                Y = Y + center_y
                XY=np.concatenate((X, Y))
                set_y[i] = XY
        
        return set_y

    def project_shapes(self, set_y, bbox, window):
        """Project shapes from original plane to a new one [-1,+1].
        
        """
        nbr = set_y.shape[0]
        nfids = int(set_y.shape[1]/2)
        std_w= window[0]
        std_h = window[1]
        for i in xrange(nbr):
            XY=set_y[i,:]
            X = XY[:nfids]
            Y = XY[nfids:]
            bb = bbox[i,:]
            center_x = bb[0] + bb[2]/2 # x_up + w/2
            center_y = bb[1] + bb[3]/2 # y_up + h/2
            X = (X - center_x) / (std_w/2)
            Y = (Y - center_y) / (std_h /2)
            XY=np.concatenate((X, Y))
            set_y[i] = XY
        
        return set_y
            
        
    def create_xml_params(self, path):
        """Create an xml file for the parameters of each hidden layers, the link and the finetune.
        
        The file will be created autmatically, but will be modified later by hand.        
        """
        parameters = Element('Parameters')
        inputHiddenLayers =  SubElement(parameters, 'InputHiddenLayers')
        link = SubElement(parameters, 'Link')
        outputHiddenLayers = SubElement(parameters, 'OutputHiddenLayers')
        finetune = SubElement(parameters, 'Finetune')
        # Input hidden layers
        # in h1
        inH1 = SubElement(inputHiddenLayers, 'InH1')
        in_pr_batch_size = SubElement(inH1, 'batch_size')
        in_pr_batch_size.text = '60'

        in_pr_epochs = SubElement(inH1, 'epochs')
        in_pr_epochs.text = '100'

        in_pr_learning_rate = SubElement(inH1, 'learning_rate')
        in_pr_learning_rate.text = '0.001'

        in_pr_growth_factor = SubElement(inH1, 'growth_factor')
        in_pr_growth_factor.text = '1.25'

        in_pr_growth_threshold = SubElement(inH1, 'growth_threshold')
        in_pr_growth_threshold.text = '5'

        in_pr_badmove_threshold = SubElement(inH1, 'badmove_threshold')
        in_pr_badmove_threshold.text = '5'
        
        in_pr_verbose = SubElement(inH1, 'verbose')
        in_pr_verbose.text = 'True'
        
        # in h2
        inH2 = SubElement(inputHiddenLayers, 'InH2')
        in_pr_batch_size = SubElement(inH2, 'batch_size')
        in_pr_batch_size.text = '60'

        in_pr_epochs = SubElement(inH2, 'epochs')
        in_pr_epochs.text = '100'

        in_pr_learning_rate = SubElement(inH2, 'learning_rate')
        in_pr_learning_rate.text = '0.001'

        in_pr_growth_factor = SubElement(inH2, 'growth_factor')
        in_pr_growth_factor.text = '1.25'

        in_pr_growth_threshold = SubElement(inH2, 'growth_threshold')
        in_pr_growth_threshold.text = '5'

        in_pr_badmove_threshold = SubElement(inH2, 'badmove_threshold')
        in_pr_badmove_threshold.text = '5'
        
        in_pr_verbose = SubElement(inH2, 'verbose')
        in_pr_verbose.text = 'True'
        
        # in h3
        inH3 = SubElement(inputHiddenLayers, 'InH3')
        in_pr_batch_size = SubElement(inH3, 'batch_size')
        in_pr_batch_size.text = '60'

        in_pr_epochs = SubElement(inH3, 'epochs')
        in_pr_epochs.text = '100'

        in_pr_learning_rate = SubElement(inH3, 'learning_rate')
        in_pr_learning_rate.text = '0.001'

        in_pr_growth_factor = SubElement(inH3, 'growth_factor')
        in_pr_growth_factor.text = '1.25'

        in_pr_growth_threshold = SubElement(inH3, 'growth_threshold')
        in_pr_growth_threshold.text = '5'

        in_pr_badmove_threshold = SubElement(inH3, 'badmove_threshold')
        in_pr_badmove_threshold.text = '5'
        
        in_pr_verbose = SubElement(inH3, 'verbose')
        in_pr_verbose.text = 'True'
        
        # link
        link_pr_batch_size = SubElement(link, 'batch_size')
        link_pr_batch_size.text = '60'

        link_pr_epochs = SubElement(link, 'epochs')
        link_pr_epochs.text = '100'

        link_pr_learning_rate = SubElement(link, 'learning_rate')
        link_pr_learning_rate.text = '0.001'

        link_pr_growth_factor = SubElement(link, 'growth_factor')
        link_pr_growth_factor.text = '1.25'

        link_pr_growth_threshold = SubElement(link, 'growth_threshold')
        link_pr_growth_threshold.text = '5'

        link_pr_badmove_threshold = SubElement(link, 'badmove_threshold')
        link_pr_badmove_threshold.text = '5'
        
        link_pr_verbose = SubElement(link, 'verbose')
        link_pr_verbose.text = 'True'

        # Input hidden layers
        # in h1
        outH1 = SubElement(outputHiddenLayers, 'OutH1')
        out_pr_batch_size = SubElement(outH1, 'batch_size')
        out_pr_batch_size.text = '60'

        out_pr_epochs = SubElement(outH1, 'epochs')
        out_pr_epochs.text = '100'

        out_pr_learning_rate = SubElement(outH1, 'learning_rate')
        out_pr_learning_rate.text = '0.001'

        out_pr_growth_factor = SubElement(outH1, 'growth_factor')
        out_pr_growth_factor.text = '1.25'

        out_pr_growth_threshold = SubElement(outH1, 'growth_threshold')
        out_pr_growth_threshold.text = '5'

        out_pr_badmove_threshold = SubElement(outH1, 'badmove_threshold')
        out_pr_badmove_threshold.text = '5'
        
        out_pr_verbose = SubElement(outH1, 'verbose')
        out_pr_verbose.text = 'True'
        
        # in h2
        outH2 = SubElement(outputHiddenLayers, 'OutH2')
        out_pr_batch_size = SubElement(outH2, 'batch_size')
        out_pr_batch_size.text = '60'

        out_pr_epochs = SubElement(outH2, 'epochs')
        out_pr_epochs.text = '100'

        out_pr_learning_rate = SubElement(outH2, 'learning_rate')
        out_pr_learning_rate.text = '0.001'

        out_pr_growth_factor = SubElement(outH2, 'growth_factor')
        out_pr_growth_factor.text = '1.25'

        out_pr_growth_threshold = SubElement(outH2, 'growth_threshold')
        out_pr_growth_threshold.text = '5'

        out_pr_badmove_threshold = SubElement(outH2, 'badmove_threshold')
        out_pr_badmove_threshold.text = '5'
        
        out_pr_verbose = SubElement(outH2, 'verbose')
        out_pr_verbose.text = 'True'
        
        # in h3
        outH3 = SubElement(outputHiddenLayers, 'OutH3')
        out_pr_batch_size = SubElement(outH3, 'batch_size')
        out_pr_batch_size.text = '60'

        out_pr_epochs = SubElement(outH3, 'epochs')
        out_pr_epochs.text = '100'

        out_pr_learning_rate = SubElement(outH3, 'learning_rate')
        out_pr_learning_rate.text = '0.001'

        out_pr_growth_factor = SubElement(outH3, 'growth_factor')
        out_pr_growth_factor.text = '1.25'

        out_pr_growth_threshold = SubElement(outH3, 'growth_threshold')
        out_pr_growth_threshold.text = '5'

        out_pr_badmove_threshold = SubElement(outH3, 'badmove_threshold')
        out_pr_badmove_threshold.text = '5'
        
        out_pr_verbose = SubElement(outH3, 'verbose')
        out_pr_verbose.text = 'True'
        
        # finetune
        batch_size = SubElement(finetune, 'batch_size')
        batch_size.text = '60'

        epochs = SubElement(finetune, 'epochs')
        epochs.text = '100'

        learning_rate = SubElement(finetune, 'learning_rate')
        learning_rate.text = '0.1'

        growth_factor = SubElement(finetune, 'growth_factor')
        growth_factor.text = '1.25'

        growth_threshold = SubElement(finetune, 'growth_threshold')
        growth_threshold.text = '5'

        badmove_threshold = SubElement(finetune, 'badmove_threshold')
        badmove_threshold.text = '5'
        
        verbose = SubElement(finetune, 'verbose')
        verbose.text = 'True'
        # saving into xml file
        rough_string = ElementTree.tostring(parameters, 'utf-8')
        reparsed = minidom.parseString(rough_string)
        xmloutput = reparsed.toprettyxml(indent="  ")
        f = open(path, "w")
        f.write(xmloutput)
        f.close()
        print xmloutput
        
        
    def parse_xml_params(self, path):
        """ Parse an xml parameters file.
        
        """
        with open(path, 'rt') as f:
            tree = ElementTree.parse(f)

        # Input hidden layers
        inputHiddenLayers =  tree.find('InputHiddenLayers')
        inputHiddenLayers_list_params=[]
        for node in list(inputHiddenLayers):
            # parse each hidden layer
            hi_params={}
            for subn in list(node):
                if subn.tag != 'verbose':
                    hi_params[subn.tag] = float(subn.text) if '.' in subn.text else int(subn.text)
                else:
                    if subn.text =='True':
                        hi_params[subn.tag] = True
                    else:
                        hi_params[subn.tag] = False
            inputHiddenLayers_list_params.append(hi_params)
        
        # Link
        link_params ={}
        link = tree.find('Link')
        for node in list(link):
            if node.tag != 'verbose':
                link_params[node.tag] = float(node.text) if '.' in node.text else int(node.text)
            else:
                if node.text =='True':
                    link_params[node.tag] = True
                else:
                    link_params[node.tag] = False
                            

        # Input hidden layers
        outputHiddenLayers =  tree.find('OutputHiddenLayers')
        outputHiddenLayers_list_params=[]
        for node in list(outputHiddenLayers):
            # parse each hidden layer
            hi_params={}
            for subn in list(node):
                if subn.tag != 'verbose':
                    hi_params[subn.tag] = float(subn.text) if '.' in subn.text else int(subn.text)
                else:
                    if subn.text =='True':
                        hi_params[subn.tag] = True
                    else:
                        hi_params[subn.tag] = False
            outputHiddenLayers_list_params.append(hi_params)
        
        # Link
        finetune_params ={}
        finetune = tree.find('Finetune')
        for node in list(finetune):
            if node.tag != 'verbose':
                finetune_params[node.tag] = float(node.text) if '.' in node.text else int(node.text)
            else:
                if node.text =='True':
                    finetune_params[node.tag] = True
                else:
                    finetune_params[node.tag] = False
        

        return inputHiddenLayers_list_params, link_params, outputHiddenLayers_list_params, finetune_params
    
    def save_unit_test(self, list_im_unit_test, output_unit_test_reproj, mean_shape_train, window,path='../data/unit_test/'):
        """ Save the output of the unit test.
        
        """
        nbr = list_im_unit_test.shape[0]
        for i in xrange(nbr):
            mtx = list_im_unit_test[i][0]
            mtx3D = np.zeros((window[0],window[1],3))
            mtx3D[:,:,0] = mtx
            mtx3D[:,:,1] = mtx
            mtx3D[:,:,2] = mtx
            
            im = Image.fromarray(np.uint8( mtx3D * 255))
            bbx = [0,0, window[0], window[1]]
            pred = output_unit_test_reproj[i]
            
            # display the mean shape
            #self.show_landmarks_unit_test( im=im, phis_pred=pred, phis_mean_train=mean_shape_train, bbox=bbx, save=True, path=path+str(i)+'.jpg')
            # don't show the mean shape.
            #self.show_only_landmarks_unit_test( im=im, phis_pred=pred, bbox=bbx, save=True, path=path+str(i)+'.jpg')
            self.show_only_landmarks_unit_test( im=im, phis_pred=mean_shape_train, bbox=bbx, save=True, path=path+str(i)+'.jpg')
            
        
        
    def read_pts_file(self, pts_path):
        """Read a pts file that contains the coordinates of the landmarks.
        
        """
        with open(pts_path) as f:
            content = f.readlines()
        content = content[3:-1] # exclude the 4 cases and the last case.
        nbr = len(content)
        X = np.zeros((nbr,1))
        Y = np.zeros((nbr,1))
        for i in xrange(nbr):
            line = content[i].split(' ')
            X[i] = np.float(line[0])
            Y[i] = np.float(line[1].replace('\n', ''))

        # remove 1 to start counting from 0 (python)        
        X = X - 1
        Y = Y - 1
        
        return X,Y
            
    def create_name_exp(self, window, nUnitsHidden_input,nUnitsLink, nUnitsHidden_output, nfids, base_name, train_link):
        """Create a name of the current experiment: wxh__in_2000_100_link_50_out_40__nfids_20_baseTarget_lfpw
        
        """
        str_h_in=""
        for i in nUnitsHidden_input:
            str_h_in = str_h_in + str(i)+'_'
        
        str_h_link=""
        if train_link:
            str_h_link = 'Trained_'
        else:
            str_h_link = 'Not_trained_'
                
        for i in nUnitsLink:
            str_h_link = str_h_link + str(i)+'_'
            
        str_h_out=""
        for i in nUnitsHidden_output:
            str_h_out = str_h_out + str(i)+'_'
            
        name_exp = str(window[0])+'x'+ str(window[1])+'__in_'+str_h_in+'link_'+ str_h_link+'out_'+str_h_out+'_nfids_'+str(nfids) + '_baseTarget_'+base_name
        exp_short_name = str(window[0])+'x'+ str(window[1])+'__in_'+str_h_in+'link_'+ str_h_link+'out_'+str_h_out
        return name_exp, exp_short_name
        
    def plot_errorbar_bbox_stats(self, stats_w, stats_h, base_names, path):
        """Plot the errorbar graph of the bbox mean+-std (of the width or hieght) of multiple sets.
        
        stats_w/h= listof tuplet (mean, std) of the w (h)
        """
        fig, axs = plt.subplots(nrows=1, ncols=2, sharex=True)
        ax_w = axs[0]
        ax_h = axs[1]
        i=1
        for data_w, data_h in zip(stats_w, stats_h):
            ax_w.errorbar(x=i, y=data_w[0], yerr=data_w[1], fmt='o', label=base_names[i-1] )
            ax_h.errorbar(x=i, y=data_h[0], yerr=data_h[1], fmt='o', label=base_names[i-1] )
            i +=1
        
        # hide the xlabels
        plt.setp(ax_w.get_xticklabels(), visible=False)
        plt.setp(ax_h.get_xticklabels(), visible=False)
        # st the x axis limits.
        ax = plt.gca()
        ax.set_xlim([-0.5, len(base_names)+1])
        ax_w.set_title('mean(hight) +- std(hieght) of bbox', fontsize=10)
        ax_h.set_title('mean(hight) +- std(hieght) of bbox', fontsize=10)
        ax_w.legend(numpoints=1, fancybox=True, shadow=True, prop={'size':6})
        ax_h.legend(numpoints=1, fancybox=True, shadow=True, prop={'size':6})
        fig.suptitle('Statistics over the bounding box width and hieght of mutliple datasets.')
        fig.savefig(path, bbox_inches='tight')   
    
    
    def overlap_graphs_sgd_tecs(self, list_saved_things, tecs_names, path_fig):
        """Overlap graphs (channels(train, valid, test ...), loss) of multiple SGD techniques, so we can 
        compare with them. The order of the techniques is: plain sgd, momentum, NAG, AdaDelta, AdaGrad, MSProp.
        
        Parameters:
            list_saved_things: list
                list of dict, each one contains the saved infos. of a technique
            tecs_names: list str
                list techniques' names
            path_fig: str
                where to save the figure.
        """
        # things to overlap: loss train, valid, test, cumulative performance
        fig, axs = plt.subplots(2,2, sharex=False, sharey=False)
        fig.tight_layout()
        ax_tr = axs[0,0]
        ax_vl = axs[0,1]
        ax_ts = axs[1,0]
        ax_cum = axs[1,1]
        
        floating = 3
        prec = "%." + str(floating) + "f"
        for (saved, name) in zip(list_saved_things, tecs_names):
            channels = saved['channels_monitor']
            loss  = saved['loss']
            loss_mean_shape = saved['loss_mean_shape']
            border_x = saved['border_x']
            dx = saved['dx']
            arch = saved['arch']
            # train loss, valid, test
            train_loss = channels['objectiveLoss']
            valid_loss = channels['validationLoss'] # we suppose it exists
            test_loss = channels['testLoss'] # we suppose it exists
            # calculation cdf loss
            x = np.arange(0,border_x,dx)
            cdf_loss_mean_shape, auc_loss_mean_shape = self.calculate_auc_of_loss( loss=loss_mean_shape,x=x, border_x=border_x, dx=dx)
            cdf_loss, auc_loss = self.calculate_auc_of_loss( loss=loss,x=x, border_x=border_x, dx=dx)
            
            # plotting
            ax_tr.plot(train_loss, label=name)
            ax_vl.plot(valid_loss, label=name)
            ax_ts.plot(test_loss, label=name)
            ax_cum.plot(cdf_loss, label=name+', AUC:'+str(prec % auc_loss) + '%')
        
        ax_cum.plot(cdf_loss_mean_shape, label='Mean shape'+', AUC:'+str(prec % auc_loss_mean_shape) + '%')
        ax_tr.grid(b=True, which='major', axis='both')
        ax_vl.grid(b=True, which='major', axis='both')
        ax_ts.grid(b=True, which='major', axis='both')
        ax_cum.grid(b=True, which='major', axis='both')
        
        ax_tr.legend(loc="upper right", bbox_to_anchor=[0, 1],
           ncol=1, shadow=True, title="Train loss", fancybox=True, prop={'size':8})
        ax_vl.legend(loc="upper right", bbox_to_anchor=[0, 1],
           ncol=1, shadow=True, title="Valid loss", fancybox=True, prop={'size':8})
        ax_ts.legend(loc="upper right", bbox_to_anchor=[0, 1],
           ncol=1, shadow=True, title="AUC test during train", fancybox=True, prop={'size':8})
        ax_cum.legend(loc="lower right", bbox_to_anchor=[0, 1],
           ncol=1, shadow=True, title="CDF", fancybox=True, prop={'size':8})
        
        fig.suptitle("Overlaped graphs for #SGD: Train loss, valid, test (during train), and CDF. Arch: "+ arch, fontsize=8)
        plt.subplots_adjust(top=0.9)
        
        fig.savefig(path_fig, bbox_inches='tight')
        return fig
    
    def plot_code_recont_for_one_set(self, data, in_aes_path):
        """Plot x-code-x_hat for one set.
        """
        x = data['x']
        code = data['code']
        reconst = data['reconstruction']
        nbr = 10 # plot only 100 samples ... just to see.
        for i in xrange(nbr):
            # x
            xx = x[i]
            sz_x = xx.shape[0]
            sqrt_sz_x = int (np.sqrt(sz_x))
            xx = np.resize(xx, sqrt_sz_x ** 2).reshape(sqrt_sz_x, sqrt_sz_x)
            # code
            c = code[i]
            sz_c = c.shape[0]
            sqrt_sz_c = int (np.sqrt(sz_c))
            c = np.resize(c, sqrt_sz_c ** 2).reshape(sqrt_sz_c, sqrt_sz_c)
            
            # reconstruction
            recon = reconst[i]
            sz_recon= recon.shape[0]
            sqrt_sz_recon = int (np.sqrt(sz_recon))
            recon = np.resize(recon, sqrt_sz_recon ** 2).reshape(sqrt_sz_recon, sqrt_sz_recon)
            # plotting ..
            fig, axs = plt.subplots(3)  
            fig.tight_layout()
            xx_ax = axs[0].imshow(xx, cmap = cm.Greys_r)
            axs[0].set_title('x')
            fig.colorbar(xx_ax, ax=axs[0])
            c_ax = axs[1].imshow(c)
            axs[1].set_title('code')
            c_ax.set_cmap('spectral')
            fig.colorbar(c_ax, ax=axs[1])
            recon_ax = axs[2].imshow(recon, cmap = cm.Greys_r)
            axs[2].set_title('x_hat')
            fig.colorbar(recon_ax, ax=axs[2])
            fig.savefig(in_aes_path+str(i)+".png",bbox_inches='tight')
            plt.close("all")
            
            
    def plot_code_recont(self, list_code_recont_input, in_aes_path, direct='in'):
        """Plot x-code-x_hat for multiple sets.
        
        """
        for (i, c_reon) in zip(xrange(len(list_code_recont_input)), list_code_recont_input):
            if direct is 'in':
                train_path=in_aes_path+'ae_in'+str(i)+'/train/'
                valid_path=in_aes_path+'ae_in'+str(i)+'/valid/'
                test_path=in_aes_path+'ae_in'+str(i)+'/test/'
            elif direct is 'out':
                train_path=in_aes_path+'ae_out'+str(i)+'/train/'
                valid_path=in_aes_path+'ae_out'+str(i)+'/valid/'
                test_path=in_aes_path+'ae_out'+str(i)+'/test/'
            else:
                raise ValueError('Unknow auto-encoder type.')
                
            if not os.path.exists(train_path):
                os.makedirs(train_path)
            if not os.path.exists(valid_path):
                os.makedirs(valid_path)
            if not os.path.exists(test_path):
                os.makedirs(test_path)
            
            self.plot_code_recont_for_one_set(data=c_reon['train'], in_aes_path=train_path)
            self.plot_code_recont_for_one_set(data=c_reon['valid'], in_aes_path=valid_path)
            self.plot_code_recont_for_one_set(data=c_reon['test'], in_aes_path=test_path)
            
    def overlap_graphs_mlp_dnn_ioda_train(self, list_saved_things, path_fig):
        """Overlap graphs (channels(train, valid), loss) of mlp, dnn and ioda, so we can 
        compare with them. There will be 2 separate figures, one for tain loss, the other for valid loss.
        
        Parameters:
            list_saved_things: list
                list of dict, each one contains the saved infos. of a configuration
            tecs_names: list str
                list techniques' names
            path_fig: str
                where to save the figure.
        """
        # 
        fig_train = plt.figure()
        fig_valid = plt.figure()

        ax_train = fig_train.add_subplot(111)
        ax_valid = fig_valid.add_subplot(111)

        
        floating = 3
        prec = "%." + str(floating) + "f"
        # MLP
        channels = list_saved_things['mlp']['channels_monitor']
        # train loss, valid, test
        train_loss = channels['objectiveLoss']['values_record']
        valid_loss = channels['validationLoss']['values_record']# we suppose it exists
        
        ax_train.plot(train_loss, label='DNN 0-0-0')
        ax_valid.plot(valid_loss, label='DNN 0-0-0')
        
        # DNN
        channels = list_saved_things['dnn']['channels_monitor']
        # train loss, valid, test
        train_loss = channels['objectiveLoss']['values_record']
        valid_loss = channels['validationLoss']['values_record'] # we suppose it exists
        
        ax_train.plot(train_loss, label='DNN 3-0-0')
        ax_valid.plot(valid_loss, label='DNN 3-0-0')
        
        # IODA
        channels = list_saved_things['ioda']['channels_monitor']
        # train loss, valid, test
        train_loss = channels['objectiveLoss']['values_record']
        valid_loss = channels['validationLoss']['values_record'] # we suppose it exists
        
        ax_train.plot(train_loss, label='IODA 1-0-3')
        ax_valid.plot(valid_loss, label='IODA 1-0-3')
    
        
        ax_train.grid(b=True, which='major', axis='both')
        ax_valid.grid(b=True, which='major', axis='both')
       
        
        ax_train.legend(loc="upper right", bbox_to_anchor=[0, 1],
           ncol=1, shadow=True, title="Train loss", fancybox=True, prop={'size':8})
        ax_valid.legend(loc="upper right", bbox_to_anchor=[0, 1],
           ncol=1, shadow=True, title="Valid loss", fancybox=True, prop={'size':8})
        
        
        ax_train.set_title('Train loss function (Mean Square Error)')
        ax_valid.set_title('Valid loss function (Mean Square Error)')
        
        
        fig_train.savefig(path_fig+'_train_loss.png', bbox_inches='tight')
        fig_valid.savefig(path_fig+'_valid_loss.png', bbox_inches='tight')
        return fig_train, fig_valid
        
        
        
## Test function
def test():

    path_text_for = 'D171.png'    
    path_text_back ='D771.png'
    # image forground/background
    im_for = misc.imread(path_text_for)
    im_back = misc.imread(path_text_back)
    size = im_for.shape
    s = size[0]    # size of the image (squared matrix)
    # number of images
    nbr_ims = 10
    train = True
    # generating the images
    data,data_labels = generate_brodatz_texture(nbr_ims, s, im_back, im_for)
    if train: # train
        sio.savemat('../data/train.mat', dict([('x_train', data), ('y_train', data_labels)]))    
    else:     # test
        sio.savemat('../data/test.mat', dict([('x_test', data), ('y_test', data_labels)]) )





































